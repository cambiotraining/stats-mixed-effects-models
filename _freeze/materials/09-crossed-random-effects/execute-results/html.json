{
  "hash": "135432a35e7e0f30a22efeeff84d154a",
  "result": {
    "markdown": "---\ntitle: \"Crossed random effects\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n\nThe previous section of course materials discussed how to fit random effects in `lme4` when there are multiple clustering variables within the dataset/experimental design, with a focus on nested random effects. \n\nThis section similarly explains how to determine the random effects structure for more complex experimental designs, but deals with the situations where the clustering variables are not nested.\n\n## What are crossed random effects?\n\nWe describe two clustering variables as \"crossed\" if they can be combined in different ways to generate unique groupings, but one of them doesn't \"nest\" inside the other.\n\nThis concept is similar to the idea of a \"factorial\" design in regular linear modelling.\n\n### Fast food example\n\nFor instance, imagine a fast food franchise is looking to perform quality control checks across different branches. In 5 randomly selected branches, testers sample 6 different items of food from the menu. They sample the same 6 items in each branch, randomly selected from the wider menu.\n\nHere, both `branch` and `menu item` would be considered random effects, but one is not nested within the other. In this situation, item A in branch 1 and item A in branch 2 are not unconnected or unique; they are the same menu item. We would want to estimate a set of 6 random intercepts/slopes for `branch`, and separately, 5 random intercepts/slopes for `menu item`.\n\n![Branch and item as crossed effects](images_mixed-effects/fastfood_design.png){width=40%}\n\nA useful rule of thumb is that if the best way to draw out your experimental design is with a table or grid like this, rather than a tree-shaped diagram, then your effects are likely to be crossed rather than nested.\n\n## Fitting crossed random effects\n\nImplementing crossed random effects in your `lme4` model is very easy. You don't need to worry about additional syntax or explicit nesting.\n\nWe'll use a behavioural dataset from a cognitive psychology study, where the classic Stroop task was administered, as a test case.\n\n### The Stroop dataset\n\nIn the Stroop task, participants are asked to identify the colour of font that a word is written in. The words themselves, however, are the names of different colours. Typically, when the font colour does not match the word itself, people are slower to identify the font colour.\n\n![The Stroop task](images_mixed-effects/stroop.png){width=70%}\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncognitive <- read_csv(\"data/stroop.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 432 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): subject, congruency\ndbl (2): item, reaction_time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n:::\n\nThis dataset contains four variables:\n\n- `subject`, of which there are 12\n- `item`, referring to task item, of which there are 36 in total\n- `congruency`, whether the colour of the font matched the word or not (congruent vs incongruent)\n- `reaction_time`, how long it took the participant to give a response (ms)\n\nOf the 36 items, 18 are congruent, and 18 are incongruent. Each subject in the study saw and responded to all 36 items, in a randomised (counterbalanced) order.\n\nOur fixed predictor is `congruency`, and we can treat both `subject` and `item` as clustering variables that create non-independent clusters amongst the 432 total observations of `reaction_time`.\nWe also consider that the reaction time change between congruent/incongruent tasks may differ across participants (i.e. we fit random slopes at the participant level).\n\nTherefore, we fit the following model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cognitive <- lmer(reaction_time ~ congruency + (1|item) +\n                        (1+congruency|subject), data=cognitive)\n\nsummary(lme_cognitive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: reaction_time ~ congruency + (1 | item) + (1 + congruency | subject)\n   Data: cognitive\n\nREML criterion at convergence: 4041.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4839 -0.6472  0.0008  0.5949  3.2836 \n\nRandom effects:\n Groups   Name                  Variance Std.Dev. Corr \n item     (Intercept)            42.2     6.496        \n subject  (Intercept)           140.4    11.847        \n          congruencyincongruent 159.4    12.626   -0.68\n Residual                       609.9    24.696        \nNumber of obs: 432, groups:  item, 36; subject, 12\n\nFixed effects:\n                      Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)            247.995      4.107  14.240   60.39  < 2e-16 ***\ncongruencyincongruent   58.222      4.860  15.581   11.98 2.87e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ncngrncyncng -0.686\n```\n:::\n:::\n\n:::\n\nIn this model, we've included a fixed effect of congruency, as well as three random effects:\n\n- random intercepts for `item`\n- random intercepts for `subject`\n- random slopes for `congruency` on `subject`\n\nWe do not fit random slopes for `congruency` on `item`, as `congruency` does not vary within individual task items.\n\nCrucially, `item` is not nested within `subject`. Item 4 for subject A is exactly the same as item 4 for subject E - we haven't given each subject their own set of items. You can see from the model output that we have therefore fitted 12 random intercepts/slopes for `subject`, and 36 random intercepts for `item`.\n\nThis allows us to capture the fixed relationship between `congruency` and `reaction_time`, with both `subject` and `item` accounted for.\n\n## Partially crossed random effects\n\nIn the example above, each participant in the study experienced each of the task items. We'd call this a fully-crossed design (or perhaps, a full factorial design). But, if each participant had only responded to a randomised subset of the task items, then we would instead say that the `item` and `subject` random effects are *partially* crossed.\n\nPartially crossed designs are common in research, such as when using the classic Latin square design, which we'll elaborate on with the next example.\n\n### The abrasion dataset\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nabrasion <- read_csv(\"data/abrasion.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): material\ndbl (3): run, position, wear\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n:::\n\nIn this experiment, four different types of `material` are being tested (A, B, C and D) for their wear, by feeding them into a wear-testing machine. \n\nThe machine could process four material samples at a time in each `run`, and it's believed that there are differences between runs. There is also evidence that the `position` within the machine might also generate some differences in wear. Therefore, four runs were made in total, with each `material` placed at each different `position` across the `run`. For each of the 16 samples, the response variable `wear` is assessed by measuring the loss of weight in 0.1mm of material over the testing period.\n\nOn first read, it might sound as if `position` and `run` are somehow nested effects, but actually, they represent a Latin square design:\n\n![Latin square design of abrasion experiment](images_mixed-effects/latin_square.png){width=30%}\n\nA Latin square is a particular type of randomised design, in which each experimental condition (in this case, materials A through D) appear once and only once in each column and row of the design matrix. This sort of randomisation might be used to randomise the layout of plants in greenhouses, or samples in wells on plates.\n\nIn the `abrasion` example, this design matrix is actually stored within the structure of the dataset itself. You can reconstruct it by looking at the raw data, or by using the following code:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nmatrix(abrasion$material, 4, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,] \"C\"  \"A\"  \"D\"  \"B\" \n[2,] \"D\"  \"B\"  \"C\"  \"A\" \n[3,] \"B\"  \"D\"  \"A\"  \"C\" \n[4,] \"A\"  \"C\"  \"B\"  \"D\" \n```\n:::\n:::\n\n:::\n\nThe four possible positions are the same across each run, meaning that `position` is not nested within `run`, but is instead crossed. Position 1 in run 1 is linked to position 1 in run 3, for instance - we wouldn't consider these to be \"unique\" positions, but would like to group them together when estimating variance in our model.\n\nBut, because it's impossible for each `material` to experience each `position` in each `run`, this is a partially crossed design rather than a fully crossed one.\n\n### Fitting partially crossed random effects\n\nThe good news is that fitting this in `lme4` doesn't require any extra knowledge or special syntax. So long as the dataset is properly coded and accurately represents the structure of the experimental design, the code is identical to fully crossed random effects.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_abrasion <- lmer(wear ~ material + (1|run) + (1|position), data = abrasion)\n\nsummary(lme_abrasion)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: wear ~ material + (1 | run) + (1 | position)\n   Data: abrasion\n\nREML criterion at convergence: 100.3\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.08973 -0.30231  0.02697  0.42254  1.21052 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n run      (Intercept)  66.90    8.179  \n position (Intercept) 107.06   10.347  \n Residual              61.25    7.826  \nNumber of obs: 16, groups:  run, 4; position, 4\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  265.750      7.668   7.475  34.656 1.57e-09 ***\nmaterialB    -45.750      5.534   6.000  -8.267 0.000169 ***\nmaterialC    -24.000      5.534   6.000  -4.337 0.004892 ** \nmaterialD    -35.250      5.534   6.000  -6.370 0.000703 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr) matrlB matrlC\nmaterialB -0.361              \nmaterialC -0.361  0.500       \nmaterialD -0.361  0.500  0.500\n```\n:::\n:::\n\n:::\n\nIf you check the output, you can see that we do indeed have 4 groups each for `run` and `position`, which is correct. The model has done what we intended, and we could now go on to look at the differences between `material`, with the nuisance effects of `run` and `position` having been accounted for.\n\n## Exercises\n\n### Penicillin {#sec-exr_penicillin}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise, we'll use the internal `Penicillin` dataset from `lme4`.\n\nThese data are taken from a study that assessed the concentration of a penicillin solution, by measuring how it inhibits the growth of organisms on a plate of agar. \n\nSix samples of the penicillin solution were taken. On each plate of agar, a few droplets of each of the six samples were allowed to diffuse into the medium. The diameter of the inhibition zones created could be measured, and is related in a known way to the concentration of the penicillin.\n\nThere are three variables:\n\n- `sample`, the penicillin sample (A through F, 6 total)\n- `plate`, the assay plate (a through x, 24 total)\n- `diameter`, of the zone of inhibition (measured in mm)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Penicillin\")\n```\n:::\n\n:::\n\nFor this exercise:\n\n1. Fit a sensible model to the data\n2. Perform significance testing/model comparison\n3. Check the model assumptions\n4. Visualise the model\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\nThis is quite a simple dataset, in that there are only two variables besides the response. But, given the research question, we likely want to consider both of these two variables as random effects.\n\nHow does that work? This is the first random-effects-only model that we've come across. (Well, technically there are still fixed effects - every time you estimate a random effect, a fixed effect will always be estimated as part of that.)\n\n#### Consider the experimental design\n\nWe have two variables for which we'd like to estimate random effects, and with no explicit fixed predictors, all that's available to us is random intercepts.\n\nThe two variables, `plate` and `sample`, are crossed in a factorial design (each of the six samples is included on each of the 24 plates). So, we want to fit these as crossed random effects.\n\n#### Fit the model\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_penicillin <- lmer(diameter ~ (1|sample) + (1|plate), data = Penicillin)\n\nsummary(lme_penicillin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: diameter ~ (1 | sample) + (1 | plate)\n   Data: Penicillin\n\nREML criterion at convergence: 330.9\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.07923 -0.67140  0.06292  0.58377  2.97959 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n plate    (Intercept) 0.7169   0.8467  \n sample   (Intercept) 3.7311   1.9316  \n Residual             0.3024   0.5499  \nNumber of obs: 144, groups:  plate, 24; sample, 6\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  22.9722     0.8086  5.4866   28.41 3.62e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nThis shows us that the average diameter of the inhibition zone is around 23mm. Looking at the random effects, there's more variance due to `sample` than there is to `plate`.\n\n#### Visualise the model\n\nWe can see these different variances by visualising the model. Here, a jagged line of best fit is drawn for each of the samples; the overall shape of the lines are the same, since we have random intercepts only. You can see that the spread within each of the lines (which represents variance for `plate`) is overall less than the spread of the lines themselves (which represents the variance for `sample`).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_penicillin), aes(x = plate, y = diameter, colour = sample)) + \n  geom_jitter(width = 0.2, height = 0) +\n  geom_line(aes(y = .fitted, group = sample))\n```\n\n::: {.cell-output-display}\n![](09-crossed-random-effects_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n:::\n\n:::\n\n:::\n\n### Politeness {#sec-exr_politeness}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise, we'll use a real dataset called `politeness`, taken from a paper by Winter & Grawunder ([2012](https://doi.org/10.1016/j.wocn.2012.08.006)).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npoliteness <- read_csv(\"data/politeness.csv\")\n```\n:::\n\n:::\n\nThe study was designed to investigate whether voice pitch is higher in polite contexts than in informal ones, and whether this effect is consistent between male and female speakers.\n\nThere are five variables in this dataset:\n\n- `subject`, the participant (6 total)\n- `gender`, treated here as a binary categorical variable (male vs female)\n- `sentence`, the sentence that was spoken (7 total)\n- `context`, whether the speaker was in a polite or informal setting\n- `pitch`, the measured voice pitch across the sentence\n\nEach participant in the study spoke each of the seven sentences twice, once in each of the two contexts.\n\nIs there a difference between vocal pitch in different contexts? Is this effect consistent for male and female speakers?\n\nTo answer this question:\n\n1. Consider which variables you want to treat as fixed and random effects\n2. Try drawing out the structure of the dataset, and think about what levels the different variables are varying at\n3. You may want to assess the quality and significance of the model to help you draw your final conclusions\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\n#### Consider the experimental design\n\nIn this dataset, there are two variables for which we might want to fit random effects: `subject` and `sentence`. The particular sets of participants and sentences have been chosen at random from the larger population of participants/speakers and possible sentences that exist.\n\nThe other two variables, `gender` and `context`, are fixed effects of interest.\n\nLet's sketch out the design of this experiment. You could choose to visualise/sketch out this design in a couple of ways:\n\n![Experimental design for voice pitch experiment #1](images_mixed-effects/politeness_design.png){width=60%}\n\n![Experimental design for voice pitch experiment #2](images_mixed-effects/politeness_design2.png){width=60%}\n\nThe `subject` and `sentence` variables are not nested within one another - they're crossed. There are 42 combinations of `subject` and `sentence`.\n\nEach of those combinations then happens twice: once for each `context`, for a total of 84 possible unique utterances. (Note that there is actually one instance of missing data, so we only have 83.)\n\nNow, `context` varies within both `subject` and `sentence` - because each subject-sentence combination is spoken twice. But `gender` does not vary within `subject` in this instance; each participant is labelled as either male or female.\n\n#### Fit a full model\n\nSo, the full possible model we could fit is the following:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_polite <- lmer(pitch ~ gender*context + (1 + gender*context|sentence)\n                   + (1 + context|subject), data = politeness)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nsummary(lme_polite)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pitch ~ gender * context + (1 + gender * context | sentence) +  \n    (1 + context | subject)\n   Data: politeness\n\nREML criterion at convergence: 762.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.5177 -0.6587 -0.0521  0.5299  3.5192 \n\nRandom effects:\n Groups   Name               Variance Std.Dev. Corr             \n sentence (Intercept)        398.96   19.97                     \n          genderM            116.03   10.77    -0.97            \n          contextpol         217.26   14.74    -0.09 -0.06      \n          genderM:contextpol 292.04   17.09     0.11 -0.10 -0.78\n subject  (Intercept)        597.74   24.45                     \n          contextpol           1.21    1.10    1.00             \n Residual                    548.79   23.43                     \nNumber of obs: 83, groups:  sentence, 7; subject, 6\n\nFixed effects:\n                   Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)         260.686     16.804    5.822  15.513 5.88e-06 ***\ngenderM            -116.195     21.618    4.306  -5.375  0.00468 ** \ncontextpol          -27.400      9.149    6.760  -2.995  0.02094 *  \ngenderM:contextpol   15.892     12.191    8.183   1.304  0.22783    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) gendrM cntxtp\ngenderM     -0.703              \ncontextpol  -0.137  0.080       \ngndrM:cntxt  0.111 -0.141 -0.723\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n:::\n\nThis full model has a singular fit, almost certainly because we don't have a sufficient sample size for 6 random effects plus fixed effects.\n\n#### Alternative (better) models\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_polite_red <- lmer(pitch ~ gender*context + (1|sentence) + (1|subject), \n                       data = politeness)\n\nsummary(lme_polite_red)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: pitch ~ gender * context + (1 | sentence) + (1 | subject)\n   Data: politeness\n\nREML criterion at convergence: 766.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.1191 -0.5604 -0.0768  0.5111  3.3352 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n sentence (Intercept) 218.3    14.77   \n subject  (Intercept) 617.1    24.84   \n Residual             637.4    25.25   \nNumber of obs: 83, groups:  sentence, 7; subject, 6\n\nFixed effects:\n                   Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)         260.686     16.348    5.737  15.946  5.7e-06 ***\ngenderM            -116.195     21.728    4.566  -5.348 0.004023 ** \ncontextpol          -27.400      7.791   69.017  -3.517 0.000777 ***\ngenderM:contextpol   15.572     11.095   69.056   1.403 0.164958    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) gendrM cntxtp\ngenderM     -0.665              \ncontextpol  -0.238  0.179       \ngndrM:cntxt  0.167 -0.252 -0.702\n```\n:::\n\n```{.r .cell-code}\nanova(lme_polite, lme_polite_red)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: politeness\nModels:\nlme_polite_red: pitch ~ gender * context + (1 | sentence) + (1 | subject)\nlme_polite: pitch ~ gender * context + (1 + gender * context | sentence) + (1 + context | subject)\n               npar    AIC    BIC  logLik deviance Chisq Df Pr(>Chisq)\nlme_polite_red    7 807.11 824.04 -396.55   793.11                    \nlme_polite       18 825.15 868.69 -394.58   789.15 3.952 11     0.9713\n```\n:::\n:::\n\n:::\n\nFitting a simpler model that contains only random intercepts, and comparing this to our more complicated model, shows no difference between the two - i.e., the simpler model is better.\n\nYou can keep comparing different models with different random effects structures, if you like, for practice - this dataset is a good sandbox for it!\n\n#### Check assumptions\n\nFor now, we're going to quickly check the assumptions of this simpler, intercepts-only model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_polite_red, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](09-crossed-random-effects_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_polite_red, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](09-crossed-random-effects_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n:::\n\nNot bad! Maybe one overly influential point (31) that deserves testing - you can try refitting the model without it, and seeing whether that changes the overall conclusions. The Q-Q plot veers off a tiny bit on the right hand side, but it's only really 3 residuals, so probably not worth worrying about.\n\nThe random intercepts look nicely normally distributed, and the posterior predictive check is quite convincing.\n\n#### Visualise the model\n\nLast but not least, let's visualise the model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_polite_red), aes(x = paste(gender, context), y = pitch, colour = gender)) +\n  geom_point(alpha = 0.7) +\n  stat_summary(fun = mean, geom = \"point\", size = 4) +\n  geom_line(aes(y = .fitted, group = paste(sentence, subject)))\n```\n\n::: {.cell-output-display}\n![](09-crossed-random-effects_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n:::\n\nBased on the model output and the visualisation, we might therefore conclude that on average, speakers do use higher pitch for polite sentences compared to informal ones. Although there is a difference in pitch between male and female speakers overall, the effect of context is similar across genders.\n\nIn the final line of code for the plot, we've included the lines of best fit for each subject-sentence combination, which have fixed gradients but random intercepts. You can view sentence-wise lines of best fit (summarised across all 6 subjects) by writing `group = sentence`, or subject-wise lines of best fit (summarised across all 7 sentences) by writing `group = subject`. These tell you a little bit more about how much variation there is between the subjects and sentences.\n\n:::\n\n:::\n\n## Summary\n\nThis section has addressed how to fit models with multiple clustering variables, in scenarios where those clustering variables are not nested with one another.\n\nThis, along with the previous section on nested random effects, helps to extend the basic linear mixed effects model that was introduced earlier in the course. It emphasises the need to understand your variables and experimental design, in order to fit a suitable model.\n\n::: {.callout-tip}\n#### Key points\n- Two random effects are \"crossed\" if they interact to create multiple unique groups/combinations (as we see in factorial experimental designs), and are not nested\n- Random effects can be fully or partially crossed\n- Crossed random effects are fitted in `lme4` by creating multiple distinct random effects structures within the model formula\n:::\n\n",
    "supporting": [
      "09-crossed-random-effects_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
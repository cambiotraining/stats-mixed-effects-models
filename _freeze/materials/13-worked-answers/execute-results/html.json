{
  "hash": "37d363d4be0baf788aecce40b6b666c8",
  "result": {
    "markdown": "---\ntitle: \"Worked answers\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n\nThis page contains worked answers to examples that are not given in the course materials.\n\n## Libraries and functions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the required packages for fitting & visualising\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(broom)\nlibrary(broom.mixed)\nlibrary(patchwork)\n```\n:::\n\n\n## Solutions\n\nView the original [Exercise -@sec-exr_solutions].\n\n::: {.callout-exercise collapse=\"true\"}\n#### Worked answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolutions <- read_csv(\"data/solutions.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 72 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): solvent, solute\ndbl (1): dissolve\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\nAn appropriate visualisation might be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(solutions, aes(x=solvent, y=dissolve, colour=solute)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](13-worked-answers_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nReading the description of the dataset carefully, the technician is not interested directly in the `solvent` variable, but instead in the fixed effect of `solute`. However, we need to adjust for the non-independence that is created by dissolving each `solute` in each `solvent` multiple times.\n\nSo, we will fit a fixed effect for `solute`, and a random effect for `solvent`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_sol_intercepts <- lmer(dissolve ~ solute + (1|solvent), data = solutions)\nsummary(lme_sol_intercepts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: dissolve ~ solute + (1 | solvent)\n   Data: solutions\n\nREML criterion at convergence: 251.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.15409 -0.67424  0.00307  0.73230  2.23245 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n solvent  (Intercept) 0.2228   0.4721  \n Residual             1.8305   1.3529  \nNumber of obs: 72, groups:  solvent, 6\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  27.5094     0.3368 15.6235  81.688  < 2e-16 ***\nsoluteB       4.3125     0.3906 64.0000  11.042  < 2e-16 ***\nsoluteC      -1.1576     0.3906 64.0000  -2.964  0.00426 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) solutB\nsoluteB -0.580       \nsoluteC -0.580  0.500\n```\n:::\n:::\n\n\nThis model can be visualised by adding to the plot above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_sol_intercepts), aes(x=solvent, y=dissolve, colour=solute)) +\n  geom_point() +\n  geom_line(aes(y=.fitted, group=solute))\n```\n\n::: {.cell-output-display}\n![](13-worked-answers_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nIf students attempt to fit a more complex model, with random slopes and intercepts, they'll receive an error informing them of singular fit. This is because either the dataset isn't large enough to support this more complex random effects structure (most likely in this case), or because the variance component of one/both of the random effects is close to zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_sol_slopes <- lmer(dissolve ~ solute + (1+solute|solvent), data = solutions)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n\n:::\n\n## Dragons (bonus questions)\n\nView the original [Exercise -@sec-exr_dragons].\n\n::: {.callout-exercise collapse=\"true\"}\n#### Worked answer (bonus questions)\n\n#### Question 1\n\nTo adapt the code to exclude `scales` as a fixed predictor, it should be dropped like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- read_csv(\"data/dragons.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): scales, mountain\ndbl (3): dragon, wingspan, intelligence\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nlme_noscales <- lmer(intelligence ~ wingspan + (1 + wingspan|mountain), data = dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Model failed to converge with 1 negative eigenvalue: -2.6e+00\n```\n:::\n:::\n\n:::\n\nThere is an interesting question to be raised here, of whether including `scales|mountain` as a random effect would constitute also estimating a fixed effect (since whenever the distribution for a random effect is estimated, this includes a mean).\n\n#### Question 2\n\nTo visualise the shrinkage,\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- read_csv(\"data/dragons.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): scales, mountain\ndbl (3): dragon, wingspan, intelligence\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nlme_noscales <- lmer(intelligence ~ wingspan + (1 + wingspan|mountain), data = dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Model failed to converge with 1 negative eigenvalue: -2.6e+00\n```\n:::\n:::\n\n:::\n\n#### Question 3\n\nThe equation in question calls for three random effects:\n\n- Random intercepts `1|mountain`, $\\beta_{0j}$\n- Random slopes for $x_1$, `wingspan|mountain`, $\\beta_{1j}$\n- Random slopes for $x_2$, `scales|mountain`, $\\beta_{2j}$\n\nWe can see in the second part of the equation that we are also estimating $\\gamma_{00}$, $\\gamma_{10}$ and $\\gamma_{20}$, which are the fixed grand average/intercept, the fixed effect of `wingspan` and the fixed effect of `scales` respectively.\n\nThe interaction term, however, is fixed only. The coefficient $\\beta_3$ has no alphabet subscripts, indicating that it does not change/takes only one value.\n\nTherefore, the model would be `intelligence ~ wingspan*scales + (1 + wingspan + scales|mountain)`.\n\n:::\n\n\n## Irrigation revisited\n\nView the original [Exercise -@sec-exr_irrigation2].\n\n::: {.callout-exercise collapse=\"true\"}\n#### Worked answer\n\nThe model of interest is as follows:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nirrigation <- read_csv(\"data/irrigation.csv\")\n\nlme_yield <- lmer(yield ~ irrigation*variety + (1|field), data = irrigation)\n```\n:::\n\n:::\n\n#### Question 1\n\nFirst, we can compare this model to the null, and see that it's significant overall (just about).\n\n::: {.panel-tabset group=\"language\"}\n##R\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_null <- lm(yield ~ 1, data = irrigation)\n\nanova(lme_yield, lm_null)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: irrigation\nModels:\nlm_null: yield ~ 1\nlme_yield: yield ~ irrigation * variety + (1 | field)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)  \nlm_null      2 89.035 90.580 -42.517   85.035                       \nlme_yield   10 88.609 96.335 -34.305   68.609 16.426  8    0.03668 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\n#### Question 2\n\nThen, we'll look at the individual predictors.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_additive <- lmer(yield ~ irrigation + variety + (1|field), data = irrigation)\n\nanova(lme_yield, lme_additive)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: irrigation\nModels:\nlme_additive: yield ~ irrigation + variety + (1 | field)\nlme_yield: yield ~ irrigation * variety + (1 | field)\n             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_additive    7 83.959 89.368 -34.980   69.959                     \nlme_yield      10 88.609 96.335 -34.305   68.609 1.3503  3     0.7172\n```\n:::\n:::\n\n\nThe interaction is not significant.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dropvar <- lmer(yield ~ irrigation + (1|field), data = irrigation)\n\nanova(lme_additive, lme_dropvar)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: irrigation\nModels:\nlme_dropvar: yield ~ irrigation + (1 | field)\nlme_additive: yield ~ irrigation + variety + (1 | field)\n             npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dropvar     6 83.586 88.221 -35.793   71.586                     \nlme_additive    7 83.959 89.368 -34.980   69.959 1.6265  1     0.2022\n```\n:::\n:::\n\n\nNeither is `variety`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dropirrig <- lmer(yield ~ variety + (1|field), data = irrigation)\n\nanova(lme_additive, lme_dropirrig)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: irrigation\nModels:\nlme_dropirrig: yield ~ variety + (1 | field)\nlme_additive: yield ~ irrigation + variety + (1 | field)\n              npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dropirrig    4 80.004 83.094 -36.002   72.004                     \nlme_additive     7 83.959 89.368 -34.980   69.959 2.0444  3     0.5632\n```\n:::\n:::\n\n\nAnd neither is `irrigation`.\n:::\n\nThis is an interesting situation, because none of the fixed effects are significant individually, but the model as a whole versus the null is.\n\nThe exercise also asks some final questions about the random intercepts term. We could try investigating them with LRT/bootstrapping methods:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_yield <- lm(yield ~ irrigation*variety, data = irrigation)\n\nanova(lme_yield, lm_yield)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: irrigation\nModels:\nlm_yield: yield ~ irrigation * variety\nlme_yield: yield ~ irrigation * variety + (1 | field)\n          npar    AIC     BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlm_yield     9 98.833 105.786 -40.416   80.833                         \nlme_yield   10 88.609  96.335 -34.305   68.609 12.223  1  0.0004719 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nA quick LRT suggests that there is a difference between the models with and without the random intercepts - in fact, quite a big one.\n\nEven if this p-value weren't quite so small, though, it's probably still be better overall to leave these random intercepts in. Without them, the model becomes a standard linear model and there's no capturing of the hierarchical structure.\n\n:::\n\n## Arabidopsis\n\nView the original [Exercise -@sec-exr_arabidopsis].\n\n::: {.callout-exercise collapse=\"true\"}\n#### Worked answer\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_arabidopsis <- lmer(total.fruits ~ nutrient + rack + status + amd + reg + \n                          (1|popu) + (1|gen), data = Arabidopsis)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n:::\n\n#### Question 1\n\nThere are several fixed effects in the model, and an error about boundary fit. For speed, we'll use the Satterthwaite approximation to investigate these fixed effects further to see if dropping them might help.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\n\n# refit with lmerTest::lmer\nlme_arabidopsis <- lmer(total.fruits ~ nutrient + rack + status + amd + reg + \n                          (1|popu) + (1|gen), data = Arabidopsis)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nanova(lme_arabidopsis)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nType III Analysis of Variance Table with Satterthwaite's method\n         Sum Sq Mean Sq NumDF  DenDF  F value    Pr(>F)    \nnutrient 155457  155457     1 612.09 135.0753 < 2.2e-16 ***\nrack      69269   69269     1 611.77  60.1874 3.627e-14 ***\nstatus     5035    2517     2 615.09   2.1874   0.11308    \namd        2897    2897     1 611.52   2.5171   0.11314    \nreg       12131    6066     2   6.33   5.2703   0.04493 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nHowever, even when we drop our insignificant `status` and/or `amd` fixed effects, our error persists.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_arabidopsis_dropfix <- lmer(total.fruits ~ nutrient + rack + reg + \n                          (1|popu) + (1|gen), data = Arabidopsis)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n:::\n\nThat's because the real issue (or at least, one of the issues) is the tiny variance component for the random intercepts on `gen`, which we can see in our model summary:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lme_arabidopsis)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: total.fruits ~ nutrient + rack + status + amd + reg + (1 | popu) +  \n    (1 | gen)\n   Data: Arabidopsis\n\nREML criterion at convergence: 6152.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.2406 -0.6075 -0.1575  0.4028  5.2390 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n gen      (Intercept)    0.00   0.000  \n popu     (Intercept)   59.32   7.702  \n Residual             1150.89  33.925  \nNumber of obs: 625, groups:  gen, 24; popu, 9\n\nFixed effects:\n                  Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)        38.1193     7.9408  16.3458   4.800 0.000185 ***\nnutrient            4.5199     0.3889 612.0855  11.622  < 2e-16 ***\nrack              -21.1149     2.7217 611.7742  -7.758 3.63e-14 ***\nstatusPetri.Plate  -6.3599     4.0107 614.2047  -1.586 0.113315    \nstatusTransplant   -5.5303     3.3058 616.9346  -1.673 0.094856 .  \namdunclipped        4.3138     2.7190 611.5249   1.587 0.113136    \nregSP              14.3758     7.6576   6.3112   1.877 0.107150    \nregSW              -6.9652     8.1588   6.5020  -0.854 0.423600    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) nutrnt rack   sttP.P sttsTr amdncl regSP \nnutrient    -0.234                                          \nrack        -0.518  0.005                                   \nsttsPtr.Plt -0.153  0.057  0.038                            \nsttsTrnspln -0.103  0.021 -0.015  0.215                     \namdunclippd -0.178  0.014  0.016 -0.037  0.002              \nregSP       -0.658 -0.002  0.008  0.033  0.007 -0.006       \nregSW       -0.615  0.003  0.005  0.035 -0.017 -0.009  0.635\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n:::\n:::\n\n:::\n\nWhy is this an issue? \n\nWell, it means that during the maximum likelihood search, the model never really left zero when estimating this parameter. Hence, our estimate of the variance is on the boundary of the possible space.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_arabidopsis_dropran <- lmer(total.fruits ~ nutrient + rack + status + amd + reg + \n                          (1|popu), data = Arabidopsis)\n```\n:::\n\n:::\n\nIndeed, when removing those random intercepts, we lose the error.\n\n#### Question 2\n\nThe problems with the diagnostic plots persist regardless of which fixed or random effects we drop. For example:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_arabidopsis_dropran, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](13-worked-answers_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_arabidopsis_dropran, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](13-worked-answers_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n:::\n\n:::\n\nThat's because of the nature of our response variable, `total.fruits`. It is not a continuous variable, but instead a count variable.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(Arabidopsis, aes(x = total.fruits)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](13-worked-answers_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n:::\n\nA linear mixed effects model is not suitable for this response variable. Instead, a generalised mixed effects model would be better - see Chapter 10 for more information.\n\n:::\n\n## Cake (bonus questions)\n\nView the original [Exercise -@sec-exr_cake].\n\n::: {.callout-exercise collapse=\"true\"}\n#### Worked answer\n\nOur current working model is as follows.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"cake\")\n\ncake <- cake %>%\n  mutate(batch = recipe:replicate)\n\nlme_cake <- lmer(angle ~ recipe*temperature + (1|batch), \n                 data = cake)\n```\n:::\n\n:::\n\n#### Question 1\n\nIf one attempts to fit the `temperature|batch` random slopes in addition to the above, the model fails.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake_slopes <- lmer(angle ~ recipe*temperature + (1 + temperature|batch), \n                 data = cake)\n```\n:::\n\n:::\n\nThis is because of a combination of things: one, the fact that `temperature` is a categorical variable or factor, and two, that there is only one measurement per `temperature` per `batch`.\n\nIn other words, there are not enough data points for each `temperature:batch` combination to estimate the random slopes.\n\nHowever, if we try using the continuous `temp` variable instead, the model runs - though arguably, the results may not be very trustworthy. We still get warnings telling us that the model failed to converge.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake_slopes2 <- lmer(angle ~ recipe*temperature + (1 + temp|batch), \n                 data = cake)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.243199 (tol = 0.002, component 1)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue\n - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio\n - Rescale variables?\n```\n:::\n:::\n\n:::\n\n#### Question 2\n\nWhat happens if we replace `temperature` with `temp` in our fixed effects (leaving off random slopes)?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake2 <- lmer(angle ~ recipe*temp + (1|batch), \n                 data = cake)\n```\n:::\n\n\nThe outputs from `anova` (using the Satterthwaite approximation) are similar, and the random effect doesn't really change.\n\nHowever, but we get a vastly different number of fixed effects when using our categorical `temperature` variable - which makes sense, because we are estimating individual group means rather than a single gradient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef(lme_cake)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          (Intercept)               recipeB               recipeC \n           33.1222222            -1.4777778            -1.5222222 \n        temperature.L         temperature.Q         temperature.C \n            6.4303299            -0.7128451            -2.3255107 \n        temperature^4         temperature^5 recipeB:temperature.L \n           -3.3512850            -0.1511858             0.4541869 \nrecipeC:temperature.L recipeB:temperature.Q recipeC:temperature.Q \n            0.0876501            -0.2327657             1.2147463 \nrecipeB:temperature.C recipeC:temperature.C recipeB:temperature^4 \n            2.6932197             2.6385602             3.0237158 \nrecipeC:temperature^4 recipeB:temperature^5 recipeC:temperature^5 \n            3.1371051            -0.6635376            -1.6252472 \n```\n:::\n\n```{.r .cell-code}\nfixef(lme_cake2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)      recipeB      recipeC         temp recipeB:temp recipeC:temp \n 2.379365079 -3.649206349 -1.941269841  0.153714286  0.010857143  0.002095238 \n```\n:::\n:::\n\n:::\n\nUsing the continuous variable, then, might be less taxing in terms of statistical power. \n\nThere is a philosophical question here, of course, about whether it's *better*. While we know that temperature in the real world is in fact a continuous variable, in this particular dataset, the different temperatures really were treated as distinct levels or categories by the original researcher.\n\n#### Question 3\n\nIf we are considering treating temperature as a random effect, then we need the categorical `temperature` version. There are, thankfully, enough levels to make this possible.\n\nWe would **not** fit this as a nested random effect, however, but instead as a crossed one. That is because each level of `temperature` occurs within every level of `batch` in a factorial design, rather than being unique.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake3 <- lmer(angle ~ recipe + (1|batch) + (1|temperature), \n                 data = cake)\n```\n:::\n\n:::\n\nThis model cannot help us compare between temperatures any more. \n\nIn theory, it *should* allow us to better investigate how breakage `angle` is affected by the `recipe`, across all possible batches and temperatures that one might use...\n\nHowever: our set of temperatures isn't really a random selection of all possible temperatures we could've baked our cakes at. This model would not extrapolate very well to temperatures below 100 or above 300 degrees, most likely. So generalisability is very poor and this defeats the purpose of fitting a random effect for the variable.\n\nWe probably don't want to fit `temperature` as a random effect, after all.\n\n:::\n\n",
    "supporting": [
      "13-worked-answers_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "6feb449197516170232c06feae7d98d4",
  "result": {
    "markdown": "---\ntitle: \"Simulating hierarchical data\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\nKey packages include several that we've already encountered, in addition to the new `MASS` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load the required packages for fitting & visualising\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(broom.mixed)\nlibrary(performance)\nlibrary(MASS)\n```\n:::\n\n:::\n\nThese bonus materials contain the code that was used for simulating the `dragons` dataset, with explanations provided.\n\nPlease note that there are many approaches to simulating data, and you are likely to find many alternative methods and tutorials elsewhere if this one does not suit you!\n\nThe code below explains how to simulate a dataset containing:\n\n- categorical and continuous fixed predictors\n- random intercepts & random slopes\n- nested & crossed random effects\n- specific variance-covariance structures\n\n## Imagine: dragons\n\nWe're going to simulate a dataset with the following structure. Random effects are shown in purple, and fixed effects in orange.\n\n![Design for `dragons` dataset](images_mixed-effects/full_dragons_design.png)\n\nThere are three levels in this dataset, i.e., we have nested random effects: \n\n- Individual `dragons` at level 1 \n- Dragons grouped within `cave` at level 2 \n- Caves grouped within `mountain` at level 3\n\nThere is also a (partially) crossed random effect of `researcher` that sits outside the hierarchy structure. This is varying between individual dragons within caves, i.e., at level 1; each dragon is observed by just one of the possible researchers.\n\nThere are two fixed effects:\n\n- `wingspan`, which varies with each individual dragon at level 1\n- `scales` colour, which varies between caves at level 2 (i.e., all dragons in a cave are the same colour)\n\n::: {.callout-note collapse=\"true\"}\n#### Other possible random slopes\n\nUnder this experimental design, there are a lot of possible random slopes, but we're not going to simulate all of them and they're not shown on the diagram above.\n\nIt would be possible, for instance, to have random slopes of `wingspan|cave`, since the fixed effect of `wingspan` varies down at level 1. This is biologically plausible - if each cave represents a dragon family, we might expect steeper or shallower relationships of `intelligence ~ wingspan` between those families. It's reasonably simple to adapt the code below to simulate this if you think it's worthwhile.\n\nIt would also be possible to fit random slopes for `scales|researcher` and `wingspan|researcher`, but neither of these are biologically very plausible. We don't really expect the `intelligence ~ wingspan + scales` relationship to vary between researchers. We'll just simulate random intercepts here, which you can think of as baseline differences in the way that the researchers assess `intelligence`.\n\nWe could even be simulating random effects of `wingspan:scales|mountain` and `wingspan:scales|researcher` - but these are probably a step too far.\n:::\n\n### Global parameters\n\nWe'll start by using the `set.seed` function. By using this, we get a reproducible result every time we run the code. \n\nIn other words, if you keep all the parameters and numbers the same and run this script over and over, you'll get the same dataset, so long as you keep the same seed.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\n```\n:::\n\n:::\n\nThen, we set the number of clusters and individuals, to give us the hierarchical structure and overall sample size.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nnr <- 6 # number of researchers (crossed random effect)\nnk <- 5 # number of mountain ranges (level 3)\nnj <- 7 # number of caves per mountain range (level 2)\nni <- 30 # number of dragons per cave (level 1)\n```\n:::\n\n:::\n\nThe total sample size, i.e., the total number of individual dragons, will be equal to `ni*nj*nk`.\n\n### Fixed effects\n\nNext comes the fixed effects. We need a coefficient for each predictor, plus a `b0` global intercept.\n\nTo keep things simple, we're going to make `wingspan` and `scales` additive, rather than including an interaction. But, if you want an interaction, you can simulate this by setting up a third beta coefficient for it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- 70 # global intercept\nb1 <- 0.5 # global slope for wingspan (fixed)\nb2 <- c(0, 12) # global slopes/offsets for scales (fixed)\n```\n:::\n\n:::\n\nSince `wingspan` is continuous, and `scales` is categorical with two levels, our beta coefficients look slightly different for the two.\n\nLastly, we're going to set a fixed standard deviation for individual dragons. We'll use this later, when we simulate our response variable `intelligence`, to add dragon-by-dragon variation - in other words, to add realistic noise to the data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nsdi <- 10\n```\n:::\n\n:::\n\n### Random effects\n\nNow comes a slightly trickier bit: setting up our random effects.\n\nRemember that when we estimate a random effect in a mixed model, we're actually estimating a *normal distribution* of the set of intercepts or slopes for that clustering variable.\n\nA normal distribution is described by two parameters - the mean, and the standard deviation (or variance). Since the mean of the distribution is being captured in our fixed effects, the part we need to specify for random effects is that standard deviation.\n\nWe do this separately for each of the random effects.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# Level 3\ntau0 <- 8 # 1|mountain\ntau1 <- 0.8 # wingspan|mountain\ntau2 <- 8 # scales|mountain\n\n# Level 2\ntau3 <- 9 # 1|cave\n\n# Crossed\ntau4 <- 10 # 1|researcher\n```\n:::\n\n:::\n\nSome of our random effects will also have some correlations between them - namely, the three random effects for `mountain`.\n\nWhen we have multiple random effects for a clustering variable, we will need the `mvrnorm` function from the `MASS` package to simulate them.\n\nThe first step in doing this is to set the correlations between the three random effects, and specify the variance-covariance matrix.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nrho01 <- 0.8*tau0*tau1 # correlation between 1|mountain and wingspan|mountain\nrho02 <- 0.4*tau0*tau2 # correlation between 1|mountain and scales|mountain\nrho12 <- 0.6*tau1*tau2 # correlation between wingspan|mountain and scales|mountain\n\nsigma <- matrix(c(tau0^2, rho01, rho02, \n                  rho01, tau1^2, rho12,\n                  rho02, rho12, tau2^2), \n                3, 3)\n```\n:::\n\n:::\n\nNow, we can simulate from a multivariate normal distribution, using the matrix we just set up.\n                \n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nmountain_randomeffs <- mvrnorm(n = nk, mu = c(rep(0, times = 3)), Sigma = sigma)\n```\n:::\n\n:::\n\nOur intercepts for `cave` and `researcher` can be estimated more simply, since there's no variance-covariance matrix to worry about (i.e., there's only one random effect for each of these clustering variables).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncave_randomeffs <- rnorm(n = nj*nk, mean = 0, sd = tau3)\n\nresearcher_randomeffs <- rnorm(n = nr, mean = 0, sd = tau4)\n```\n:::\n\n:::\n\n### Create predictor variables\n\nThe next thing to do is to generate the values for our predictor and clustering variables.\n\n#### Random predictor variables\n\nHere, we create unique IDs for each `mountain` range and each `cave`. (Importantly, these cave IDs do not repeat between mountain ranges - in other words, our dataset will be explicitly nested.)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nmountain <- rep(LETTERS[1:nk],each = ni*nj)\n\ncave <- as.factor(rep(1:(nj*nk), each = ni))\n```\n:::\n\n:::\n\nWe'll also create unique IDs for each `dragon`. This variable will essentially just repeat the row numbers, since we have one dragon per row of the dataset, but it's useful for completeness (or if we ever decided to mess with our dataset later on, such as pivoting it to wide format, or removing rows).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragon <- 1:(nj*ni*nk)\n```\n:::\n\n:::\n\nAnd finally, we create unique IDs for each `researcher`. We're just going to vary this randomly at the level of individual dragons.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nresearcher <- sample(rep(c(\"AP\", \"RS\", \"LO\", \"CN\", \"YW\", \"KH\"), \n                         length.out = ni*nj*nk))\n```\n:::\n\n:::\n\n#### Fixed predictor variables\n\nFor `wingspan`, we sample a continuous variable from a uniform distribution (you could choose a different distribution if you wanted). This varies at level 1, so we want a new wingspan value for each individual dragon.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nwingspan <- round(runif(nj*ni*nk, 13, 96))\n```\n:::\n\n:::\n\nFor `scales`, we have two possible values, `chromatic` or `metallic`.\n\nAccording to our design, `scales` varies at the level of `cave`, level 2. We code that like this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nscales <- rep(c(rep(\"chromatic\", times = ni), rep(\"metallic\", times = ni)), \n              length.out = ni*nj*nk)\n```\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### Coding `scales` at different levels\n\nIf you wanted `scales` to instead vary only between `mountain` ranges - i.e., each mountain range only contains one scale colour - you can code that by adjusting the `times` argument in the `rep` function:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nscales_lvl3 <- rep(c(rep(\"chromatic\", times = ni*nj), \n                     rep(\"metallic\", times = ni*nj)), \n                   length.out = ni*nj*nk)\n```\n:::\n\n:::\n\nOr, if you wanted `scales` to vary down at the individual level, you can drop the `times` argument entirely, using similar code to what was used to generate the `researcher` variable above:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nscales_lvl1 <- sample(rep(c(\"chromatic\", \"metallic\"), length.out = ni*nj*nk))\n```\n:::\n\n:::\n:::\n\n#### Checking the structure\n\nIt's useful here to pause and look at the variables you've created, all together, to check that the structure matches what you were expecting to see.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(dragon, wingspan, scales, mountain, cave, researcher)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,050 × 6\n   dragon wingspan scales    mountain cave  researcher\n    <int>    <dbl> <chr>     <chr>    <fct> <chr>     \n 1      1       29 chromatic A        1     LO        \n 2      2       55 chromatic A        1     CN        \n 3      3       80 chromatic A        1     YW        \n 4      4       15 chromatic A        1     RS        \n 5      5       74 chromatic A        1     AP        \n 6      6       41 chromatic A        1     RS        \n 7      7       26 chromatic A        1     KH        \n 8      8       95 chromatic A        1     CN        \n 9      9       24 chromatic A        1     AP        \n10     10       27 chromatic A        1     YW        \n# ℹ 1,040 more rows\n```\n:::\n:::\n\n::: \n\nSo far, so good.\n\n### Create response variable and collate dataframe\n\nWe'll simulate our response variable `intelligence` in two separate steps. \n\nFirst, we produce a set of theoretical **average** values for `intelligence` for the given values of our predictors that exist in our dataset, without any individual noise/error.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\naverage_response <- b0 + b1*wingspan + model.matrix(~ 0 + scales) %*% b2 +\n  model.matrix(~ 0 + mountain + wingspan:mountain + scales:mountain) %*% as.vector(mountain_randomeffs) +\n  model.matrix(~ 0 + cave) %*% as.vector(cave_randomeffs) +\n  model.matrix(~ 0 + researcher) %*% as.vector(researcher_randomeffs)\n```\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### The model.matrix method\n\nWe're using the `model.matrix` function, and the `%*%` matrix multiplier operator, wherever we have a categorical variable. The function sets up a table of 1s and 0s, and our coefficients (or random deviations) are added only where there is a 1. \n\nMatrix multiplication has its own set of rules, going well beyond mixed models or even the R language, and we won't go into detail here. But you can run these lines of code in RStudio to get a closer look at what's happening as we multiply these matrices together.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nview(model.matrix(~ 0 + mountain + wingspan:mountain + scales:mountain))\n\nview(as.vector(mountain_randomeffs))\n\nview(model.matrix(~ 0 + mountain + wingspan:mountain + scales:mountain) %*% as.vector(mountain_randomeffs))\n```\n:::\n\n:::\n\n:::\n\nThe second step is to sample our our **actual** values of our `intelligence` variable.\n\nFor each value of `intelligence`, we sample from a normal distribution where the mean is the `average_response` that we generated just above, and where the standard deviation is the individual `sdi` that we set along with the other global parameters. \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nintelligence <- round(rnorm(nj*ni*nk, average_response, sdi))\n```\n:::\n\n:::\n\nThis gives us a set of values for `intelligence` that are based on the model we've specified, but include normally-distributed random errors, as we would expect to see in actual data.\n\nLast but not least, let's build the dataframe so we can look at the data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- tibble(dragon, intelligence, wingspan, scales, \n                  mountain, cave, researcher)\n```\n:::\n\n::: \n\n### Fit and visualise simulated model\n\nHaving simulated based on a model, it's useful to check that the model does in fact do a decent job of capturing all the sources of variation we built it with.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_test <- lmer(intelligence ~ wingspan + scales + \n                   (1 + wingspan + scales|mountain) + \n                   (1|cave) +\n                   (1|researcher),\n                 data = dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00371082 (tol = 0.002, component 1)\n```\n:::\n:::\n\n::: \n\nThis model formula mirrors exactly the formula we used to create `average_response`, so if everything has gone well, we should get back out the parameters we put in. \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lme_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nintelligence ~ wingspan + scales + (1 + wingspan + scales | mountain) +  \n    (1 | cave) + (1 | researcher)\n   Data: dragons\n\nREML criterion at convergence: 7915.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6091 -0.6781 -0.0016  0.6512  3.3544 \n\nRandom effects:\n Groups     Name           Variance Std.Dev. Corr     \n cave       (Intercept)    95.0826  9.7510            \n researcher (Intercept)    41.9570  6.4774            \n mountain   (Intercept)    82.3890  9.0768            \n            wingspan        0.5689  0.7543   0.73     \n            scalesmetallic 74.9855  8.6594   0.69 1.00\n Residual                  93.4913  9.6691            \nNumber of obs: 1050, groups:  cave, 35; researcher, 6; mountain, 5\n\nFixed effects:\n               Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)     70.4551     5.4260  6.3134  12.985 8.65e-06 ***\nwingspan         0.3796     0.3376  3.9941   1.125   0.3238    \nscalesmetallic  16.2370     5.1378  4.7225   3.160   0.0271 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) wngspn\nwingspan    0.538        \nscalesmtllc 0.190  0.752 \noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00371082 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\n# extract values for fixed effects\nfixef(lme_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)       wingspan scalesmetallic \n    70.4551141      0.3796023     16.2370446 \n```\n:::\n\n```{.r .cell-code}\n# extract values for random effects\nranef(lme_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$cave\n   (Intercept)\n1    0.5593680\n2    1.8027555\n3    1.5570649\n4   -4.8934710\n5    8.4916245\n6   -1.6639623\n7    2.6979231\n8   16.6340092\n9    3.2337177\n10   6.0797569\n11 -11.6061454\n12   4.6314090\n13 -13.6247454\n14 -10.6554106\n15  -7.9644316\n16  -5.4236484\n17  14.0456340\n18  10.4002966\n19   9.1821409\n20 -12.3287784\n21 -18.3445087\n22   0.1489425\n23  14.1946474\n24   5.8127781\n25   6.9357773\n26  -2.1854496\n27 -14.6533699\n28   8.0774363\n29  -3.7075641\n30 -15.0386221\n31   1.1639208\n32  -2.1632850\n33   9.2598154\n34   0.7652433\n35  -1.4208688\n\n$researcher\n   (Intercept)\nAP    4.950110\nCN   -7.794784\nKH   -3.156414\nLO    8.844382\nRS    2.428189\nYW   -5.271482\n\n$mountain\n  (Intercept)   wingspan scalesmetallic\nA   -1.561117 -0.5952300      -7.093171\nB   -1.679598  0.1140412       1.505426\nC  -11.871687 -0.8950826      -9.962033\nD   10.725514  0.4099539       4.171622\nE    4.386888  0.9663175      11.378156\n\nwith conditional variances for \"cave\" \"researcher\" \"mountain\" \n```\n:::\n:::\n\n::: \n\nThis looks right. The fixed effects are more or less correct, and we have the right number of random coefficients for each clustering variable.\n\nWe did get a singular fit error when fitting the model. Usually this is an issue of power; either the effect sizes are very small, or there aren't enough data points. You might want to alter the global parameters to deal with this.\n\n#### Check assumptions\n\nBased on how we simulated the dataset, a linear mixed model should be perfectly appropriate - but it's worth checking that nothing went wrong.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_test, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](12-simulating-hierarchical-data_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_test, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](12-simulating-hierarchical-data_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n:::\n\n::: \n\n#### Visualise data\n\nThis is perhaps the most important step. Do your simulated data actually look the way you expect?\n\nIn this first plot, we'll look at the dataset in a single plot, with separate lines of best for each cave.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_test), aes(y = intelligence, x = wingspan, \n                              colour = mountain, linetype = scales)) +\n  geom_point() +\n  geom_line(aes(y = .fitted, group = paste(mountain, cave)))\n```\n\n::: {.cell-output-display}\n![](12-simulating-hierarchical-data_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: \n\nAs intended, each cave is either chromatic or metallic. Within mountains, metallic dragons are on average cleverer than chromatic ones, but the slope of the `intelligence ~ wingspan` relationship is roughly similar. The slope of the relationship, however, does vary a fair bit between mountains.\n\nIn this second plot, we facet by `researcher`, to see what effects there are.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_test), aes(y = intelligence, x = wingspan, \n                              colour = mountain, linetype = scales)) +\n  facet_wrap(~ researcher) +\n  geom_point() +\n  geom_line(aes(y = .fitted, group = paste(mountain, cave, researcher)))\n```\n\n::: {.cell-output-display}\n![](12-simulating-hierarchical-data_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: \n\nQuite minimal. The overall effect of all predictors is very similar within each panel, but is very subtly shifted up or down along the y axis, which aligns with what we simulated.\n\nFinally, let's look at things by mountain, with individual lines of best fit for each cave.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_test), aes(y = intelligence, x = wingspan, \n                              colour = researcher, linetype = scales)) +\n  facet_wrap(~ mountain) +\n  geom_point() +\n  geom_line(aes(y = .fitted, group = paste(cave, researcher)))\n```\n\n::: {.cell-output-display}\n![](12-simulating-hierarchical-data_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n:::\n\nHere, we're able to see the `scales|mountain` random effect a bit more clearly. In some mountain ranges, all our caves are bunched quite closely together, while in others, it's clear that the metallic caves are on average full of more intelligent dragons, though the slopes are all parallel.\n\nThis all fits nicely with what we simulated. But, if we decided that some of the effects should be stronger or weaker, we could re-set some of our parameters, and re-simulate without needing to do very much extra work.\n\n## Using simulation for power analysis\n\nA useful application for simulation of this type in mixed effects modelling is to perform power analysis.\n\nOne way you might wish to implement this is via the package `simr`, which is designed to perform power analysis by simulation for mixed effects models fitted using `lme4`. \n\nThis package can be used to investigate power *a posteriori*, i.e., once you already have a dataset and want to report on the power of your model. Perhaps more helpfully, you can also couple it with the simulation technique taught above to perform a power analysis *a priori*, i.e., to determine what sample size you might need for a prospective study.\n\nThe following links provide some information on how to do this:\n\n[This worked example](https://humburg.github.io/Power-Analysis/simr_power_analysis.html) shows a step-by-step process of running a power analysis in `simr`, as well as a slightly different way to simulate a dataset using the package.\n\n[This methods paper](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504) gives another, shorter tutorial.\n\n## Summary\n\nSimulating hierarchical datasets requires several distinct steps, and importantly, you need to know the structure of your dataset and experimental design very clearly before you can do so. Making even small changes to the code could result in a predictor varying at a different level of the hierarchy to what you initially intended.\n\nWhen simulating random effects in particular, you need to consider the relationship between those effects, by specifying a variance-covariance matrix. For more information on variance-covariance matrices, see the other bonus materials in this course.\n\n::: {.callout-tip}\n#### Key Points\n\n- Consider the experimental design carefully to ensure you simulate the right number of fixed and random effects\n- Set global parameters and fixed beta coefficients first\n- When simulating values of your clustering variables, use explicit nesting\n- Random effects can be simulated using `rnorm`, or `mvrnorm` when there are multiple random effects for a single clustering variable\n- Matrix multiplication can be used to simulate categorical variables, including random effects\n- Use a two-step procedure for simulating your response variable\n:::\n\n",
    "supporting": [
      "12-simulating-hierarchical-data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
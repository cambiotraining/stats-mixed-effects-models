{
  "hash": "6588cb2debdc8d45b3486712cc00b691",
  "result": {
    "markdown": "---\ntitle: \"Checking assumptions\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n\nAs with all statistical models, mixed effects models make certain assumptions about the dataset and the population it's drawn from. If these assumptions are not well met, then any results we get from our model must be taken with a huge grain of salt.\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\nWe'll be using the `performance` package in R to visually check assumptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install and load the package\ninstall.packages(\"performance\")\nlibrary(performance)\n```\n:::\n\n:::\n\n## What are the assumptions?\n\nThe assumptions of a linear mixed effects model - which we've been dealing with so far in the course - are very similar to the assumptions of a standard linear model, and include all the things you're likely used to seeing:\n\n- Continuous response variable\n- Independence of data points (beyond the non-independence that we have accounted for with our random effects)\n- Linearity in the relationship between the predictor(s) and the response\n- Residuals are normally distributed\n- Residuals have equality of variance\n\nAnd, though it isn't a \"formal\" assumption in the strictest sense, we also want to ensure that there aren't any overly influential data points.\n\nBecause we now have random effects in our model, there are a few additional assumptions that we make:\n\n- The coefficients of the random effects are normally distributed\n- Random effects are not influenced by any of the other predictors\n\n## Testing these assumptions\n\nThe first two of our assumptions - continuous response variable and independence - can't be tested just by examining the dataset or residuals. These two assumptions fit within a broader idea of \"choose the right model\", which requires you as a researcher to think carefully about your experimental design.\n\nThe rest of our assumptions can be assessed using the same method that we use for a standard linear regression analysis: visualisation.\n\nLet's look at our `sleepstudy` dataset again. Here is the full model that we fitted to those data:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"sleepstudy\")\n\nlme_sleep <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n```\n:::\n\n:::\n\nNow, let's visualise it. We could create each of the plots by hand if we wished (using the `broom.mixed` package to augment our dataset), but thankfully there exists a much quicker method, using an R package called `performance`.\n\n::: {.callout-tip}\nThe `performance` package contains a bunch of functions that allow us to test the quality of our model. For the purposes of visualisation, we'll use `check_model`, but I encourage you to explore this package in more detail as there's a lot more to it (it's super helpful for evaluating the performance of generalised linear models and Bayesian models, as well as mixed models).\n\nNote that you might also need to install and/or load the `see` package in order to use the `performance` package.\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_sleep, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](checking-assumptions_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n:::\n\nHere, we've specified just the 4 residual plots that we're used to seeing. Perhaps the one exception is the plot for influential observations, which looks a little different to the residuals vs leverage or Cook's distance plots that you might be used to. On this plot, there are 4 data points labelled in red which fall really far outside our dashed contour lines (8, 57, 60 and 131). This tells us that we might want to re-examine these points, perhaps by excluding them from the dataset, fitting a new linear mixed model, and seeing whether our conclusions are still the same.\n\nThe linearity and homogeneity of variance plots look alright, overall, although there's some indication that our influential points might be causing a bit of drama there too. There's some snaking in the Q-Q plot that suggests our residuals have a \"heavy-tailed\", or leptokurtic, distribution.\n\n### Normality of random effects\n\nThe other important assumption to check via visualisation is the normality of our random effects estimates.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_sleep, check = \"reqq\")\n```\n\n::: {.cell-output-display}\n![](checking-assumptions_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n:::\n\nWe have two random effects in our model - the random slopes and random intercepts. For each of these, a separate normal Q-Q plot has been constructed. Notice that there are 18 points in each of our Q-Q plots here, which correspond to our 18 subjects (because `Subject` was our clustering variable for our random effects).\n\nThis lets us evaluate whether our set of coefficients for these random effects are normally distributed. In other words - does the set of y-intercepts and the set of gradients that were generated (one for each of our subjects) appear to have been sampled from a normal underlying distribution? Here, it looks like they do, which is excellent news.\n\n### Posterior predictive check\n\nOne of the other plots that is offered in this package is called the posterior predictive check. It's quite a nice option to include, as it can give you an overall idea of how good a job your model does in predicting your data.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_sleep, check = \"pp_check\")\n```\n\n::: {.cell-output-display}\n![](checking-assumptions_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\nHere, the function has run a set of simulations for us, using the linear mixed model we created. Each of these simulated datasets, created from our model, is shown on the plot by a thin blue line (as you can see, many simulations have been run).\n\nThe green line then shows us our current dataset. If the green line shows the same sort of pattern as all the thinner blue lines, this indicates good overall model fit.\n\nFor this dataset, it really isn't bad at all for the most part! However, our dataset (the green line) does have a bit of a \"dip\" or \"dent\" that doesn't seem to be occurring in very many of our blue lines. This could potentially indicate that our model is a bit too simple, i.e., there is some other important variable that we've not factored in here; or it could simply be a result of random noise.\n\n::: {.callout-tip, collapse=\"true\"}\n### Changing plotting colours in check_model\n\nIf you find the green, blue and red default colours in `check_model` to be a little too similar to each other for your liking, there is an optional `colours` argument in the function that you can add. For instance, you could change the green to a yellow, by adding this to the `check_model` function: `colors = c(\"#fada5e\", \"#1b6ca8\", \"#cd201f\")`.\n:::\n\n## Exercise\n\nLet's revisit the `dragons` dataset, and the minimal model that we chose in the previous section based on significance testing:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- read_csv(\"data/dragons.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): scales, mountain\ndbl (3): dragon, wingspan, intelligence\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nlme_dragons_dropx <- lmer(intelligence ~ wingspan + scales + \n                            (1 + wingspan|mountain), \n                            data=dragons)\n```\n:::\n\n:::\n\nWhat do you think of the diagnostic plots below for this model? Feel free to discuss with a neighbour.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_dragons_dropx, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](checking-assumptions_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_dragons_dropx, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](checking-assumptions_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n:::\n\n## Summary\n\n::: {.callout-tip}\n#### Key Points\n\n- Linear mixed effects models have the same assumptions as standard linear models\n- Mixed models also make assumptions about the distribution of random effects\n- The `performance` package in R can be used to assess whether these assumptions are met using diagnostic plots\n:::",
    "supporting": [
      "checking-assumptions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
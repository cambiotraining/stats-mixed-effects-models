{
  "hash": "48403a5e519bebe3628e0612388da302",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Significance & model comparison\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n\n## Libraries and functions\n\n::: {.callout-note collapse=\"true\"}\n## Click to expand\n\nWe'll primarily be using the `lmerTest` package for performing certain types of significance tests. The `pbkrtest` package is also introduced.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\nlibrary(pbkrtest)\n```\n:::\n\n:::\n\n## The problem \n\nUnlike standard linear models, p-values are not calculated automatically for a mixed effects model in `lme4`, as you may have noticed in the previous section of the materials. There is a little extra work and thought that goes into testing significance for these models.\n\nThe reason for this is the inclusion of random effects, and the way that random effects are estimated. When using partial pooling to estimate the random effects, there is no way to precisely determine the number of **degrees of freedom**. \n\nThis matters, because we need to know the degrees of freedom to calculate p-values in the way we usually do for a linear model (see the drop-down box below if you want a more detailed explanation for this).\n\n::: {.callout-note collapse=\"true\"}\n#### Degrees of freedom & p-values\n\nThe degrees of freedom in a statistical analysis refers to the number of observations in the dataset that are free to vary (i.e., free to take any value) once the necessary parameters have been estimated. This means that the degrees of freedom varies with both the sample size, and the complexity of the model you've fitted.\n\nWhy does this matter? Well, each test statistic (such as F, t, chi-square etc.) has its own distribution, from which we can derive the probability of that statistic taking a certain value. That's precisely what a p-value is: the probability of having collected a sample with this particular test statistic, if the null hypothesis were true. \n\nCrucially, the exact shape of this distribution is determined by the number of degrees of freedom. This means we need to know the degrees of freedom in order to calculate the correct p-value for each of our test statistics.\n:::\n\nHowever, when we fit a mixed effects model, we may still want to be able to discuss significance of a) our overall model and b) individual predictors within our model.\n\n## Overall model significance\n\nLikelihood ratio tests (LRTs) are used to compare goodness-of-fit, or deviance, between two models in order to produce p-values. They don't require us to know the degrees of freedom of those models.\n\nOne use of an LRT is to check the significance of our model as a whole, although we'll revisit the LRT in later sections of this page as well.\n\n::: {.callout-note collapse=\"true\"}\n#### What makes this test a \"likelihood ratio\"? \n\nRemember that mixed effects models are fitted by maximising their likelihood, which is defined as the joint probability of the sample given a particular set of parameters (i.e., how likely is it that this particular set of data points would occur, given a model with this equation?).\n\nEach distinct mixed model that is fitted to a given dataset therefore has its own value of likelihood. It will also, therefore, have its own value of deviance. Deviance is defined as the difference in log-likelihoods between a candidate model, and the hypothetical perfect \"saturated\" model for that dataset.\n\nSo, when we want to compare two models, we can calculate the ratio of their individual likelihoods (which is mathematically equivalent to the difference of their deviances, because of how logarithms work). This ratio can be thought of as a statistic in its own right, and approximately follows a chi-square distribution. \n\nTo determine whether this ratio is significantly different from 1, we calculate the degrees of freedom for the analysis - which is equal to the difference in the number of parameters between the two models we're comparing - to find the corresponding chi-square distribution, from which we can then calculate a p-value.\n:::\n\nLet's try this out on the trusty `sleepstudy` dataset. We create both our candidate model, `lm_sleep`, and a null model, `lm_null` (note, we have to do this using the `lm` function rather than `lmer`)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"sleepstudy\")\n\nlme_sleep <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n\nlm_null <- lm(Reaction ~ 1, data = sleepstudy)\n```\n:::\n\n:::\n\nThen, we use the old faithful `anova` function to compare our candidate model to the null model, by calling them one after the other. Note that we have to call our candidate model first; if you list the null model first, you'll get an error.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep, lm_null)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: sleepstudy\nModels:\nlm_null: Reaction ~ 1\nlme_sleep: Reaction ~ Days + (1 + Days | Subject)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlm_null      2 1965.0 1971.4 -980.52   1961.0                         \nlme_sleep    6 1763.9 1783.1 -875.97   1751.9 209.11  4  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nThis table gives us the $\\chi^2$ statistic (i.e., the likelihood ratio) and an associated p-value. Here, the $\\chi^2$ is large and the p-value small, meaning that our model is significantly better than the null.\n\nA helpful, intuitive way to think about this test is: for the increase in complexity of my candidate model (vs the null model), has the deviance of the model decreased significantly? Or: given the number of predictors in my model, has the goodness-of-fit improved significantly from the null?\n\n::: {.callout-note collapse=\"true\"}\n#### Refitting using ML (instead of ReML)\n\nNote the warning/information message R provides when we use the `anova` function this way: \"refitting model(s) with ML (instead of REML)\".\n\nR, or more specifically the `anova` function, has done something helpful for us here. For reasons that we won't go into too much (though, feel free to ask if you're curious!), we cannot use LRTs to compare models that have been fitted with the ReML method, even though this is the standard method for the `lme4` package. So we must refit the model with ML.\n\n(Incidentally, we could have chosen to fit the models manually with ML, if we'd wanted to. The `lmer` function takes an optional `REML` argument that we can set to FALSE - it's set to TRUE by default. But letting the `anova` function do it for us is much easier!)\n:::\n\n## Fixed effects\n\nIn addition to asking about the model as a whole, we often want to know about individual predictors. Because it's simpler, we'll talk about fixed predictors first.\n\nThere are multiple methods for doing this. We'll step through the some of the most popular in a bit of detail:\n\n- Likelihood ratio tests\n- F-tests using approximations of degrees of freedom\n- t-to-z approximations (Wald tests)\n- Bootstrapping\n\n### Method 1: Likelihood ratio tests (LRTs)\n\nAs we mentioned above, LRTs are useful for comparing the model as a whole to the null - but they can also be used to investigate individual predictors.\n\nCrucially, we are only able to use this sort of test when one of the two models that we are comparing is a \"simpler\" version of the other, i.e., one model has a subset of the parameters of the other model. \n\nSo while we could perform an LRT just fine between two models `Y ~ A + B + C` and `Y ~ A + B + C + D`, to investigate the effect of `D`, or between any model and the null (`Y ~ 1`), we would not be able to use this test to compare `Y ~ A + B + C` and `Y ~ A + B + D`.\n\n![Two ways to use likelihood ratio tests](images_mixed-effects/LRT_schematic.png){width=70%}\n\nLet's use an LRT to test the fixed effect of `Days` in our `sleepstudy` example. First, we'll fit a random-effects-only model (we do this by replacing `Days` with `1`, to indicate no fixed effects).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_sleep_random <- lmer(Reaction ~ 1 + (1 + Days|Subject),\n                   data = sleepstudy)\n```\n:::\n\n:::\n\nThen we use `anova` to compare them, again putting our more complex model first.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep, lme_sleep_random)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: sleepstudy\nModels:\nlme_sleep_random: Reaction ~ 1 + (1 + Days | Subject)\nlme_sleep: Reaction ~ Days + (1 + Days | Subject)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_sleep_random    5 1785.5 1801.4 -887.74   1775.5                         \nlme_sleep           6 1763.9 1783.1 -875.97   1751.9 23.537  1  1.226e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nThis output tells us that, for the reduction in the number of parameters (i.e., removing `Days`), the difference in deviances is significantly big. In other words, a fixed effect of `Days` is meaningful when predicting reaction times.\n\n### Method 2: Approximation of the degrees of freedom\n\nThis method is perhaps the most intuitive for those coming from a linear modelling background. Put simply, it involves making an educated guess about the degrees of freedom with some formulae, and then deriving a p-value as we usually would. \n\nThis lets us obtain p-values for any t- and F-values that are calculated, with just the one extra step compared to what we're used to with linear models.\n\nFor this approach, we will use the companion package to `lme4`, a package called `lmerTest`.\n\n::: {.callout-note collapse=\"true\"}\n#### lmerTest\n\nThe package provides a modified version of the `lmer()` function, one that can approximate the number of degrees of freedom, and thus provide estimated p-values.\n\nIf you have `lmerTest` loaded, R will automatically default to its updated version of the `lmer()` function, and perform the degrees of freedom approximation as standard. (You can prevent it from doing so by typing `lme4::lmer()` instead.)\n:::\n\nLet's look again at our random slopes & intercepts model for the `sleepstudy` dataset as a test case. We'll refit the model once we've loaded the new package.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\n\nlme_sleep <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n```\n:::\n\n:::\n\nThe new version of the `lmer` function fits a very similar model object to before, except now it contains the outputs of a number of calculations that are required for the degrees of freedom approximation. By default, `lmerTest` uses the Satterthwaite approximation, which is appropriate for mixed models that are fitted using either MLE or ReML, making it pretty flexible.\n\nWe'll use the `anova` function from the `lmerTest` package to produce an analysis of variance table (R will default to using this version of the function unless told otherwise). This gives us an estimate for the F-statistic and associated p-value for our fixed effect of `Days`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nType III Analysis of Variance Table with Satterthwaite's method\n     Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nDays  30031   30031     1    17  45.853 3.264e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\n::: {.callout-note collapse=\"true\"}\n#### F-statistics vs t-statistics\n\nIf you were to look at the summary for our new `lme_sleep` model, you'd notice some t-statistics and p-values appearing next to the fixed effects. These are **not quite the same** as the F-statistics and p-values that we've extracted using the `anova` function.\n\nIn fact, this odd distinction between t-statistics and F-statistics is not unique to mixed models; you might remember it from linear modelling. The t-statistics are what we call \"Wald tests\" (more coming up on those in the next section) and test the null hypothesis that the coefficient $\\beta = 0$ for that predictor. This might not sound *too* dissimilar from what an analysis of variance F-test is assessing - and for continuous predictors, the result is usually very similar. But for a categorical predictor, you will see separate Wald tests for each pairwise comparison against the reference group, while you would only see a single F-statistic for the lot.\n:::\n\n#### Using the Kenward-Roger approximation\n\nAlthough the Satterthwaite approximation is the `lmerTest` default, another option called the Kenward-Roger approximation also exists. It's less popular than Satterthwaite because it's a bit less flexible (it can only be applied to models fitted with ReML). \n\nIf you wanted to switch to the Kenward-Roger approximation, you can do it easily by specifying the `ddf` argument:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep, ddf = \"Kenward-Roger\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nType III Analysis of Variance Table with Kenward-Roger's method\n     Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nDays  30031   30031     1    17  45.853 3.264e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nIn reality, though, chances are that you'll just stick with the Satterthwaite default if you plan to use approximations for your own analyses. Statisticians have debated the relative merits of Satterthwaite versus Kenward-Roger, but the differences only really tend to emerge under specific conditions. Here, it's given us the same result.\n\n### Method 3: t-to-z approximations\n\nThis is a more unusual method, and another form of approximation. You'll see this less often, but it's included here for completeness. \n\nThis method involves making use of the Wald t-values, which are reported as standard in the `lme4` output.\n\nSpecifically, we can choose to treat these t-values as if they were z-scores instead, if our sample size is considered large enough. And, because z-scores are standardised, we don't need any degrees of freedom information to derive a p-value - we can just read them directly out of a table (or get R to do it for us).\n\n::: {.callout-note collapse=\"true\"}\n#### The logic of using z-scores instead\n\nA z-score is different from a statistic such as t or F. They're standardised, because they're measured in standard deviations - i.e., a z-score of 1.3 tells you that you are 1.3 standard deviations away from the mean. \n\nThis is helpful for deriving a p-value without degrees of freedom, but it raises the question: why is it okay to treat t-values as z-scores? \n\nThe logic here is that the t distribution actually begins to approximate (i.e., match up with) the z distribution as the sample size increases. Officially, when the sample size is infinite, the two distributions are identical. So, with a sufficiently large sample size, we can \"pretend\" or \"imagine\" that the Wald t-values are actually z-distributed, giving us p-values. \n:::\n\nUnfortunately, there are no formal guidelines to tell you whether your dataset is \"large enough\" to do this. It will depend on the number and type of predictors in your model. Plus, the t-to-z approximation is considered to be \"anti-conservative\" - in other words, there's a higher chance of false positives than with other methods.\n\nSome researchers adapt the t-to-z approximation approach a little to help with this; instead of explicitly calculating p-values, they instead use a rule of thumb that any Wald t-value greater than 2 is large enough to be considered significant. This is quite a strict threshold, so it can help to filter out some of the false positives or less convincing results.\n\nCalculating the p-value for a z-score can be done quickly in R using the `pnorm` function. We include the z-score (or, here, the t-value that we are treating as a z-score) as the value for our argument `q`. To make this a two-tailed test, we have to set `lower.tail` to FALSE, and multiply the answer by 2.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lme_sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Reaction ~ Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***\nDays          10.467      1.546  17.000   6.771 3.26e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n```\n\n\n:::\n\n```{.r .cell-code}\n2*pnorm(q = 6.771, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.278953e-11\n```\n\n\n:::\n:::\n\n:::\n\nIf we input the t-value for our `Days` fixed effect, we can see that it gives us a very small p-value. This p-value of 1.28 x 10^-11^ is quite a bit smaller than the one that our Satterthwaite degrees of freedom approximation provided (3.26 x 10^-6^) - an example of how this t-to-z approximation is more generous. However, in this case it's very clear that the `Days` effect definitely is significant, whichever way we test it, so it's perhaps not a concern.\n\n### Method 4: Bootstrapping\n\nNow, we get a little bit more technical. \n\nEntire pages of course materials could be dedicated to bootstrapping and simulation methods. These ideas go well beyond linear mixed models. But, now is not the time for all that.\n\nWe're going to look at one implementation of bootstrapping for mixed models, as an example, but if you're curious then a good place to start follow-up reading is [this excellent resource](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-significance-of-random-effects).\n\nThe specific option we'll look at is performing parametric bootstrapping via the `PBmodcomp` function from the `pbkrtest` package.\n\nThis method involves:\n\n1. Simulating a bunch of datasets (specifically, based on the \"reduced\" or less complex model)\n2. For each simulated dataset, fit both models\n3. For each simulated dataset, compute the difference in deviances between the two models, to provide a distribution of differences in deviances\n4. Compare this distribution to the actual/observed difference in deviances\n\nThe syntax is very similar to the `anova` function, but you also set a seed. \n\n(This is something that's often done when simulating in general; it ensures that each time you run the code, you'll get the same set of numbers, so long as you use the same seed. You can choose whatever number you like.)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_sleep, lme_sleep_random, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 17.71 sec; samples: 1000; extremes: 0;\nRequested samples: 1000 Used samples: 996 Extremes: 0\nlarge : Reaction ~ Days + (1 + Days | Subject)\nReaction ~ 1 + (1 + Days | Subject)\n         stat df   p.value    \nLRT    23.537  1 1.226e-06 ***\nPBtest 23.537     0.001003 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nIt takes several seconds, because running 1000 simulations and fitting 2000 models isn't instantaneous. You may also get a bunch of warnings (they've been suppressed here for these course materials, but don't be alarmed if they appear for you when running this example).\n\nBut, as you can see, the p-value it produces is not necessarily the same as the one produced by a standard LRT.\n\n### Choosing the \"right\" method\n\nSeveral methods have been discussed here. Lots of researchers favour either the F-tests by degrees of freedom approximation, or the likelihood ratio test (LRT) for fixed effects, because they're relatively easy to implement - hence why we've spent slightly more time on them. \n\nIf we had to choose, we personally favour the LRT, because it's generalisable to any type of model that's fitted with maximum likelihood estimation, making it a very useful addition to a researcher's statistical toolkit.\n\nThose with more coding or theoretical background, however, might feel strongly that bootstrapping is always a more appropriate method for deriving p-values. And they might well be right. There's no strict answers once we get this far beyond the standard linear model.\n\nIt's worth noting that there's nothing stopping you using more than one approach when it comes to testing your own models, and \"triangulating\" the results to help you determine how robust your conclusions are. \n\n## Random effects\n\nWith fixed effects under our belt, let's now move to thinking about random effects.\n\nThere is a broader philosophical question to be asked here: what does it even mean for a random effect to be \"significant\"?\n\nRemember that a random effect is not a single coefficient. It's a measure of the distribution across a set of clusters or groups. Quite often, we include a random effect simply to account for it, to better represent our design, not because we want to treat it as a \"predictor\" in the traditional sense.\n\nPerhaps a better way to think about it is: **is my model better with or without this random effect?**\n\nOr even: **is there a need to test significance at all?**\n\nWe'll talk through a few different approaches:\n\n- Using LRTs (with caveats)\n- Using AIC/BIC (also with caveats)\n- Bootstrapping\n- Not testing at all\n\n### Method 1: Using LRTs\n\nThe most common method that you'll see used for judging whether random effects improve a model is the trusty LRT.\n\n::: {.callout-warning collapse=\"true\"}\n#### The major caveat with LRTs for random effects\n\nThough you'll see LRTs used often for random effects, *technically* this doesn't provide great estimates.\n\nWhen we run such a test, we're essentially asking whether the variance of our chosen random effect is equal to zero (i.e., our null hypothesis is $\\sigma^2 = 0$). But, as a statistican might point out, 0 is \"on the boundary of the feasible space\" - in other words, 0 is the lowest possible value that the variance could ever be.\n\nBecause of this, the various approximations to distributions that we rely on for the maths of an LRT to work, kind of break down. The result is that the p-values calculated for LRTs are very conservative, i.e., too large/strict.\n\nIn the simplest case, testing simple random effects one at a time, the p-value is approximately twice as large as it should be. And the problem gets worse when testing multiple correlated random effects (see bonus materials for more info on these correlations).\n\nThis doesn't stop people using them for this purpose, and it doesn't have to stop you. But it's something you should really be aware of if you choose this method.\n:::\n\nThe approach is much the same as for fixed effects: construct two nested models, with and without the effect of interest. \n\nThen, use the `anova` function to perform the LRT.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_sleep_intercepts <- lmer(Reaction ~ Days + (1|Subject),\n                   data = sleepstudy)\n\nanova(lme_sleep, lme_sleep_intercepts)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: sleepstudy\nModels:\nlme_sleep_intercepts: Reaction ~ Days + (1 | Subject)\nlme_sleep: Reaction ~ Days + (1 + Days | Subject)\n                     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_sleep_intercepts    4 1802.1 1814.8 -897.04   1794.1                     \nlme_sleep               6 1763.9 1783.1 -875.97   1751.9 42.139  2  7.072e-10\n                        \nlme_sleep_intercepts    \nlme_sleep            ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nOnce again, there is a significant difference between the two models, as seen by our small p-value. This tells us that the random slopes of `Days|Subject` is meaningful, and makes a difference in our model.\n\nYou can even use the `anova` function to compare models with and without random effects entirely, i.e., compare a linear mixed model to a linear model. \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_sleep <- lm(Reaction ~ Days, data = sleepstudy)\n\nanova(lme_sleep, lm_sleep)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: sleepstudy\nModels:\nlm_sleep: Reaction ~ Days\nlme_sleep: Reaction ~ Days + (1 + Days | Subject)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlm_sleep     3 1906.3 1915.9 -950.15   1900.3                         \nlme_sleep    6 1763.9 1783.1 -875.97   1751.9 148.35  3  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nMake sure you call the linear mixed model (i.e., the more complex model) first, because you will get an error if you put the two models the wrong way around here.\n\n### Method 2: AIC/BIC values\n\nSome researchers use model comparison procedures, such as stepwise elimination, to decide whether or not to keep or drop certain random effects from their models.\n\nAs you may have noticed in the outputs from all of the LRTs above, the `anova` function automatically provides Akaike information criterion (AIC) and Bayesian information criterion (BIC) values for the different nested models.\n\nFor instance, when comparing `lm_sleep` and `lme_sleep` above, we can see that the linear model has larger AIC/BIC values (and greater deviance, i.e., worse goodness-of-fit) than the linear mixed model with our random slopes & intercepts in it.\n\n::: {.callout-warning collapse=\"true\"}\n#### The same caveat as with LRTs\n\nUsing AIC/BIC to make decisions about random effects is subject to **the same caveat as for LRTs**: the values you get for these information criteria end up being overly conservative.\n\nIn other words, AIC/BIC values can give an underestimation of the importance or use of a random effect in a linear mixed model, perhaps leading you to drop it even if it's helpful.\n:::\n\n### Method 3: Bootstrapping\n\nAs we did above for the fixed effects, we can use parametric bootstrapping to investigate random effects.\n\nIt works in exactly the same way: feed in two models, one with and one without the random effect that you're interested in testing, and don't forget to pick a value to set the seed.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_sleep, lme_sleep_intercepts, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 14.42 sec; samples: 1000; extremes: 0;\nlarge : Reaction ~ Days + (1 + Days | Subject)\nReaction ~ Days + (1 | Subject)\n         stat df   p.value    \nLRT    42.139  2 7.072e-10 ***\nPBtest 42.139     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nOnce again, you may get a long list of warnings as it simulates and fits models to a bunch of different datasets.\n\n### Method 4: Not testing at all\n\nThis might seem like a bit of an odd concept, especially placed where it is at the end of a page all about significance testing.\n\nAnd of course, we're not advocating for throwing all the possible random effects into an overly complicated model and just accepting whatever numbers fall out. You're still aiming for parsimony, and your model should still represent what's actually going on in your experimental design.\n\nBut, many people - including those with far more experience in mixed models than us - argue that you shouldn't drop a random effect simply because a p-value or AIC/BIC value tells you so. If that random effect is truly important in representing the design and structure of your dataset, then your model is better served by containing it. \n\nIn other words, it's meaningful because of the experimental design, not because of the numbers that come out of your model.\n\nThis philosophical stance is particularly applicable in situations where you're including random effects simply to account for the hierarchical, non-independent structure in your data, because you're interested in the overall or average trends.\n\n::: {.callout-note}\n#### A final thing to add...\n\nSome of the people who take this stance (including authors of some of the packages we've used) might argue that significance is no more important, or is even less important, than the *uncertainty* of the random effects. How confident are we that we've estimated the variance correctly? What are the confidence intervals within which the variance falls?\n\nNow, that really is a can of worms we're not going to open here, but you might be interested to know that packages exist for computing these confidence intervals; `lme4` even comes with a function for it.\n\nIf you're curious, you could start some follow-up reading [here](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#inference-and-confidence-intervals).\n:::\n\n## Exercises\n\n### Dragons revisited {#sec-exr_dragons2}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nLet's return to the dataset from a previous exercise, [Exercise -@sec-exr_dragons].\n\nPreviously, we fit a mixed model to this dataset that included response variable `intelligence`, fixed effects of `wingspan`, `scales` and `wingspan:colour`, and two random effects: random intercepts `1|mountain`, and random slopes for `wingspan|mountain`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- read_csv(\"data/dragons.csv\")\n\nlme_dragons <- lmer(intelligence ~ wingspan*scales + (1 + wingspan|mountain), \n                    data = dragons)\n```\n:::\n\n:::\n\nUse likelihood ratio tests to assess:\n\n- whether the model above is significant versus the null model\n- whether the fixed effects are significant\n\nIf you're feeling adventurous, you can also:\n\n- use LRTs, AIC and/or bootstrapping to assess the random effects, and compare the results\n- use other methods to assess the significance of the fixed effects, and compare the results\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\nLet's start by using an LRT to test the overall significance of our model. We'll construct a null model, and then use `anova` to compare it to our model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dragons_null <- lm(intelligence ~ 1, data = dragons)\n\nanova(lme_dragons, lme_dragons_null)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_null: intelligence ~ 1\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_dragons_null    2 1997.0 2003.6 -996.51   1993.0                         \nlme_dragons         8 1647.8 1674.2 -815.92   1631.8 361.18  6  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nIt's significant. Something in our model is doing something helpful. A really good start!\n\nNext, we'll use LRTs to test the significance of our individual fixed effects. \n\nWe'll start with the interaction. To test this, we'll build an additive model, and compare it to our original full model. For the models to be comparable, we'll keep the random effects structure the same.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dragons_dropx <- lmer(intelligence ~ wingspan + scales + (1 + wingspan|mountain), \n                          data = dragons)\n\nanova(lme_dragons, lme_dragons_dropx)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_dropx: intelligence ~ wingspan + scales + (1 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                  npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropx    7 1647.2 1670.3 -816.60   1633.2                     \nlme_dragons          8 1647.8 1674.2 -815.92   1631.8 1.3648  1     0.2427\n```\n\n\n:::\n:::\n\n:::\n\nThe test isn't significant. This tells us that the `wingspan:scales` interaction wasn't doing anything meaningful in this model.\n\nNow, we're going to test the main effects of `scales` and `wingspan` by constructing two new models and comparing them to our additive model. (In this way, we're performing something a little bit like a stepwise elimination procedure.)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nFirst, we'll test the interaction term by comparing our additive model to our original full model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dragons_dropscale <- lmer(intelligence ~ wingspan + (1 + wingspan|mountain), \n                              data = dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Model failed to converge with 1 negative eigenvalue: -2.6e+00\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_dragons_dropx, lme_dragons_dropscale)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_dropscale: intelligence ~ wingspan + (1 + wingspan | mountain)\nlme_dragons_dropx: intelligence ~ wingspan + scales + (1 + wingspan | mountain)\n                      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropscale    6 1673.6 1693.3 -830.78   1661.6                     \nlme_dragons_dropx        7 1647.2 1670.3 -816.60   1633.2 28.359  1  1.008e-07\n                         \nlme_dragons_dropscale    \nlme_dragons_dropx     ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nlme_dragons_dropwing <- lmer(intelligence ~ scales + (1 + wingspan|mountain), \n                            data = dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.003579 (tol = 0.002, component 1)\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_dragons_dropx, lme_dragons_dropwing)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_dropwing: intelligence ~ scales + (1 + wingspan | mountain)\nlme_dragons_dropx: intelligence ~ wingspan + scales + (1 + wingspan | mountain)\n                     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropwing    6 1653.5 1673.2 -820.73   1641.5                     \nlme_dragons_dropx       7 1647.2 1670.3 -816.60   1633.2 8.2604  1   0.004052\n                       \nlme_dragons_dropwing   \nlme_dragons_dropx    **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nBoth of these tests come out as significant. This suggests that both fixed effects for `wingspan` and `scales` are making meaningful contributions to our model.\n\nComfortingly, this aligns with what we see in an analysis of variance table using a Satterthwaite degrees of freedom approximation, which shows overall that there seem to be main effects though no significant interaction. The p-values are not the same - we wouldn't expect them to be, they're calculated very differently - but it's a relief that the overall effect is robust across methods:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_dragons)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF   DenDF F value  Pr(>F)   \nwingspan        3059.90 3059.90     1   3.992 16.8644 0.01483 * \nscales          1923.44 1923.44     1 188.766 10.6008 0.00134 **\nwingspan:scales  242.84  242.84     1 188.380  1.3384 0.24878   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nWe would draw the same overall conclusion using t-to-z approximations as well (using the t-values, extracted from the output of the `summary` function). Excellent news.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nThe interaction term:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pnorm(q = -1.157, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.752728\n```\n\n\n:::\n:::\n\n\nThe main effect of scales:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pnorm(q = 3.256, lower.tail = FALSE) # scales main effect\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001129938\n```\n\n\n:::\n:::\n\n\nThe main effect of wingspan:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pnorm(q = 4.244, lower.tail = FALSE) # wingspan main effect\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.195704e-05\n```\n\n\n:::\n:::\n\n:::\n\nAnd finally, you can check the results from a parametric bootstrap (once again, the warnings have been suppressed here), which yet again agree with the prior tests:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_dragons, lme_dragons_dropx, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 19.91 sec; samples: 1000; extremes: 243;\nRequested samples: 1000 Used samples: 988 Extremes: 243\nlarge : intelligence ~ wingspan * scales + (1 + wingspan | mountain)\nintelligence ~ wingspan + scales + (1 + wingspan | mountain)\n         stat df p.value\nLRT    1.3653  1  0.2426\nPBtest 1.3653     0.2467\n```\n\n\n:::\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_dragons_dropx, lme_dragons_dropscale, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 17.65 sec; samples: 1000; extremes: 0;\nRequested samples: 1000 Used samples: 985 Extremes: 0\nlarge : intelligence ~ wingspan + scales + (1 + wingspan | mountain)\nintelligence ~ wingspan + (1 + wingspan | mountain)\n         stat df   p.value    \nLRT    28.364  1 1.005e-07 ***\nPBtest 28.364     0.001014 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_dragons_dropx, lme_dragons_dropwing, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 18.04 sec; samples: 1000; extremes: 13;\nRequested samples: 1000 Used samples: 844 Extremes: 13\nlarge : intelligence ~ wingspan + scales + (1 + wingspan | mountain)\nintelligence ~ scales + (1 + wingspan | mountain)\n         stat df  p.value   \nLRT    8.2598  1 0.004053 **\nPBtest 8.2598    0.016568 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nOn the basis of all of these results, you might choose to refine your model slightly, eliminating the unhelpful `wingspan:scales` interaction and making `lme_dragons_dropx` the working minimal model.\n\nWe can visualise that like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dragons, aes(x = wingspan, y = intelligence, colour = scales)) +\n  facet_wrap(vars(mountain)) +\n  geom_point() +\n  geom_line(data = augment(lme_dragons_dropx), aes(y = .fitted))\n```\n\n::: {.cell-output-display}\n![](06-significance-and-model-comparison_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n:::\n\nWhat about the random effects, then?\n\nLet's test them first with LRTs (and AIC/BIC).\n\n::: {.panel-tabset group=\"language\"}\n## R\n\nWe construct two new models, one with each of the random effects dropped. We keep the fixed effects structure the same, so that the models are otherwise comparable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dragons_dropslope <- lmer(intelligence ~ wingspan*scales + (1|mountain), \n                              data = dragons)\n\nlme_dragons_dropint <- lmer(intelligence ~ wingspan*scales + (0 + wingspan|mountain), \n                            data = dragons)\n```\n:::\n\n\nThen, we use the `anova` function to compare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropint)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_dropint: intelligence ~ wingspan * scales + (0 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropint    6 1643.9 1663.7 -815.95   1631.9                     \nlme_dragons            8 1647.8 1674.2 -815.92   1631.8 0.0691  2     0.9661\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropslope)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: dragons\nModels:\nlme_dragons_dropslope: intelligence ~ wingspan * scales + (1 | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropslope    6 1737.9 1757.7 -862.95   1725.9                     \nlme_dragons              8 1647.8 1674.2 -815.92   1631.8 94.057  2  < 2.2e-16\n                         \nlme_dragons_dropslope    \nlme_dragons           ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nThese results would seem to suggest that the random slopes are significant, but the random intercepts are not. \n\nThis is borne out by the change in information criteria values. When we remove `1|mountain`, both AIC and BIC decrease (by 3.9 and 10.5 respectively), suggesting improvement in the model quality - remember that lower values are better for these criteria. In contrast, when we remove `wingspan|mountain`, both AIC and BIC increase by a large amount (by 90.1 and 83.5 respectively), suggesting we have worsened the quality of the model.\n\nBut, we know that the LRT p-values and AIC/BIC values for random effects aren't always great, so let's compare to a parametric bootstrap just to be sure.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_dragons, lme_dragons_dropint, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 14.73 sec; samples: 1000; extremes: 646;\nRequested samples: 1000 Used samples: 761 Extremes: 646\nlarge : intelligence ~ wingspan * scales + (1 + wingspan | mountain)\nintelligence ~ wingspan * scales + (0 + wingspan | mountain)\n         stat df p.value\nLRT    0.0691  2  0.9661\nPBtest 0.0691     0.8491\n```\n\n\n:::\n\n```{.r .cell-code}\npbkrtest::PBmodcomp(lme_dragons, lme_dragons_dropslope, seed = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBootstrap test; time: 14.85 sec; samples: 1000; extremes: 0;\nlarge : intelligence ~ wingspan * scales + (1 + wingspan | mountain)\nintelligence ~ wingspan * scales + (1 | mountain)\n         stat df   p.value    \nLRT    94.057  2 < 2.2e-16 ***\nPBtest 94.057     0.000999 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nThe p-values are indeed different, but not different enough to change our conclusions. \n\nHowever: we would likely want to be cautious about dropping the random intercepts from this model. What does a random slopes-only model mean in biological terms? In this instance, it would suggest that all `mountain` ranges have the same baseline `intelligence` level when `wingspan` is very small/zero, but the rate of change based on their size (`intelligence ~ wingspan`) does vary between ranges. \n\nIs this biologically plausible? We're not tracking dragons across multiple time points here, so we can't say for sure, but this could reflect dragons in some mountain ranges learning quicker as they grow than dragons elsewhere due to better schools, in which case it might be plausible that they're all born with the same baseline `intelligence`. But it could also reflect different species of dragon living in each mountain range, in which case, it's very plausible that `intelligence` on average could vary between ranges (even if we're not observing it in this particular dataset).\n\nDo we need to reduce the number of random parameters in our model? Our dataset is not huge, for the number of variables we're testing. But our additive `lm_dragons_dropx` model with both random effects is converging sensibly. It might not be necessary.\n\n:::\n\n:::\n\n### Irrigation revisited {#sec-exr_irrigation2}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nOnce again, we'll return to a dataset from the previous section of the course, this time [Exercise -@sec-exr_irrigation], and the model we fitted to it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nirrigation <- read_csv(\"data/irrigation.csv\")\n\nlme_yield <- lmer(yield ~ irrigation*variety + (1|field), data = irrigation)\n```\n:::\n\n:::\n\nCompare and contrast the results from likelihood ratio test and other methods, to assess:\n\n- the significance of the model overall\n- the significance of the fixed predictors\n\nThere's no worked answer for this exercise, but you can use the code from the `sleepstudy` and `dragons` examples to scaffold your work.\n\nConsider also the random intercepts. If an LRT or bootstrap indicated that the random effect wasn't significant, would you drop the intercepts from the model? Why/why not? Feel free to chat to a neighbour or trainer to help make your decision.\n\n:::\n\n## Summary\n\nThis section showcases multiple methods of performing significance testing and model comparison for mixed effects models - but also introduces a broader debate as to when and how significance testing is actually useful for this type of model.\n\nIf you're interested in doing further reading on the different methods for significance testing, then [this article](https://link.springer.com/article/10.3758/s13428-016-0809-y) has a nice comparison of the methods discussed above, including how they perform in terms of type I (false positive) error rates.\n\n::: {.callout-tip}\n#### Key Points\n\n- Calculating p-values for mixed effects models is tricky, and must be done differently to a standard linear model, because there is no precise number of degrees of freedom\n- For fixed effects, p-values can be calculated using F-tests with approximations of degrees of freedom, likelihood ratio tests, t-to-z approximations or bootstrapping\n- For random effects, options are more limited to likelihood ratio tests or bootstrapping methods\n- AIC/BIC values and stepwise elimination procedures can also be used to provide information about fixed and/or random effects in a linear mixed model, and to aid with model comparison\n- Likelihood ratio tests and AIC/BIC values in particular rely heavily on the concept of deviance (goodness-of-fit)\n:::\n\n",
    "supporting": [
      "06-significance-and-model-comparison_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
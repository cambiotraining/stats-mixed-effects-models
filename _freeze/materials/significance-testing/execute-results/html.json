{
  "hash": "fef990206d07bf892d2753fa4b86de79",
  "result": {
    "markdown": "---\ntitle: \"Significance testing\"\noutput: html_document\n---\n\n\n\n::: {.cell}\n\n:::\n\n\nUnlike standard linear models, p-values are not calculated automatically for a mixed effects model in `lme4`, as you may have noticed in the previous section of the materials. There is a little extra work and thought that goes into testing significance for these models.\n\nThe reason for this is the inclusion of random effects, and the way that random effects are estimated. When using partial pooling to estimate the random effects, there is no way to precisely determine the number of **degrees of freedom**. \n\nThis matters, because we need to know the degrees of freedom to calculate p-values in the way we usually do for a linear model (see the drop-down box below if you want a more detailed explanation for this).\n\n::: {.callout-note collapse=\"true\"}\n#### Degrees of freedom & p-values\n\nThe degrees of freedom in a statistical analysis refers to the number of observations in the dataset that are free to vary (i.e., free to take any value) once the necessary parameters have been estimated. This means that the degrees of freedom varies with both the sample size, and the complexity of the model you've fitted.\n\nWhy does this matter? Well, each test statistic (such as F, t, chi-square etc.) has its own distribution, from which we can derive the probability of that statistic taking a certain value. That's precisely what a p-value is: the probability of having collected a sample with this particular test statistic, if the null hypothesis were true. \n\nCrucially, the exact shape of this distribution is determined by the number of degrees of freedom. This means we need to know the degrees of freedom in order to calculate the correct p-value for each of our test statistics.\n:::\n\nHowever, when we fit a mixed effects model, we still want to be able to discuss significance of a) our overall model and b) individual predictors within our model.\n\n## Multiple methods for assessing significance\n\nSo, how do we get around the degrees of freedom problem?\n\nThere are several methods for doing this, and different methods for fixed vs random effects. But we'll work through some of the most popular together, including:\n\n- Likelihood ratio tests\n- Approximations of degrees of freedom\n- t-as-z approximations\n\n## Method 1: Approximation of the degrees of freedom\n\nWe'll start by exploring the most intuitive of the three methods.\n\nPut simply, this approach involves making an educated guess about the degrees of freedom with some formulae, and then deriving a p-value as we usually would. This lets us obtain p-values for any t- and F-values that are calculated, with just the one extra step compared to what we're used with linear models.\n\n::: {.callout-important}\n#### A major caveat\n\nDespite being very simple and intuitive, there is a downside: this method does not provide p-values for random effects, only for fixed effects.\n:::\n\nFor this approach, we will use the companion package to `lme4`, a package called `lmerTest`. It provides an \"updated\" version of the `lmer()` function, one that can approximate the number of degrees of freedom, and thus provide estimated p-values.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\n```\n:::\n\n:::\n\nNow, whenever you use the `lmer()` function, R will automatically use the version from the `lmerTest` package. (You can prevent it from doing so by typing `lme4::lmer()` instead.)\n\nThe `lmerTest` package uses the Satterthwaite approximation by default. This particular approximation is appropriate for mixed models that are fitted using either MLE or ReML, making it pretty flexible.\n\nUsing this new package, let's look again at our random slopes & intercepts model for the `sleepstudy` dataset:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# refit the random intercepts & random slopes model\n# using lmerTest instead of lme4\nlme_sleep2 <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n```\n:::\n\n:::\n\nThe new version of the `lmer` function fits a very similar model object to before, except now it contains the outputs of a number of calculations that are required for the Satterthwaite approximation already built in.\n\nThis means that we can now use the `anova` function from the `lmerTest` package to produce an analysis of variance table. This gives us an estimate for the F-statistic and associated p-value for our fixed effect of `Days`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nType III Analysis of Variance Table with Satterthwaite's method\n     Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nDays  30031   30031     1    17  45.853 3.264e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\n### Using the Kenward-Roger approximation\n\nAlthough the Satterthwaite approximation is the `lmerTest` default, another option called the Kenward-Roger approximation also exists. It's less popular than Satterthwaite because it's a bit less flexible (it can only be applied to models fitted with ReML). \n\nIf you wanted to switch to the Kenward-Roger approximation, you can do it easily by specifying the `ddf` argument:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep2, ddf=\"Kenward-Roger\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nType III Analysis of Variance Table with Kenward-Roger's method\n     Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nDays  30031   30031     1    17  45.853 3.264e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nIn reality, though, chances are that you'll just stick with the Satterthwaite default if you plan to use approximations for your own analyses. Statisticians have debated the relative merits of Satterthwaite vs K-R, but the differences only really tend to emerge under specific conditions. Here, it's given us the same result.\n\n## Method 2: Likelihood ratio tests (LRTs)\n\nWhen introducing degrees of freedom approximations above, I mentioned a major caveat to them: they only work for testing fixed effects. So, now we're going to look at an alternative method, which we can use for testing both fixed *and* random effects.\n\nThis is a more flexible and general method for assessing significance, called a likelihood ratio test (LRT). An LRT is used to compare goodness-of-fit between two models, and crucially, doesn't require us to know the degrees of freedom of those model(s).\n\n::: {.callout-note collapse=\"true\"}\n#### What makes this test a \"likelihood ratio\"? \n\nRemember that mixed effects models are fitted by maximising their likelihood, which is defined as the joint probability of the sample given a particular set of parameters (i.e., how likely is it that this particular set of data points would occur, given a model with this equation?).\n\nEach distinct mixed model that is fitted to a given dataset therefore has its own value of likelihood. When we want to compare two models, we can calculate the ratio of their individual likelihoods. This ratio can be thought of as a statistic of its own (akin to the t- or F-statistic), and approximately follows a chi-square distribution. \n\nTo determine whether this ratio is significantly different from 1, we calculate the degrees of freedom for the analysis - which is equal to the difference in the number of parameters in the model - to find the corresponding chi-square distribution, from which we can then calculate a p-value.\n:::\n\nCrucially, we are only able to use this sort of test when one of the two models that we are comparing is a \"simpler\" version of the other, i.e., one model has a subset of the parameters of the other model. \n\nSo while we could perform an LRT just fine between these two models: `Y ~ A + B + C` and `Y ~ A + B + C + D`, or between any model and the null (`Y ~ 1`), we would not be able to use this test to compare `Y ~ A + B + C` and `Y ~ A + B + D`.\n\nWe can use LRTs to assess the significance of pretty much any individual effect (random or fixed) in our mixed effects model, by comparing versions of the model with versus without that predictor. We can also use LRTs to assess the fit of the model as a whole, by comparing it to the null model.\n\n### Running LRTs\n\nSince LRTs involve making a comparison between two models, we must first decide which models we're comparing, and check that one model is a \"subset\" of the other.\n\nLet's return to our `sleepstudy` dataset, and fit two models - one with random intercepts only, and one with random intercepts and random slopes.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# load lme4, and the dataset from the package\nlibrary(lme4)\ndata(\"sleepstudy\")\n\n# fit the random intercepts model\nlme_sleep1 <- lmer(Reaction ~ Days + (1|Subject),\n                   data = sleepstudy)\n\n# fit the random intercepts & slopes model\nlme_sleep2 <- lmer(Reaction ~ Days + (1 + Days|Subject),\n                   data = sleepstudy)\n```\n:::\n\n:::\n\nNow, we will use the old faithful `anova` function. Rather than putting just one model in, however, we will include both the models that we want to compare, listing them one after another.\n\n::: {.callout-note}\nNote the warning/information message R provides when we use the `anova` function this way: \"refitting model(s) with ML (instead of REML)\".\n\nR, or more specifically the `anova` function, has done something helpful for us here. For reasons that we won't go into too much (though, feel free to ask if you're curious!), we cannot use LRTs to compare models that have been fitted with the ReML method, even though this is the standard method for the `lme4` package. So we must refit the model with ML.\n\n(Incidentally, we could have chosen to fit the models manually with ML, if we'd wanted to. The `lmer` function takes an optional `REML` argument that we can set to FALSE - it's set to TRUE by default. But letting the `anova` function do it for us is much easier!)\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_sleep2, lme_sleep1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: sleepstudy\nModels:\nlme_sleep1: Reaction ~ Days + (1 | Subject)\nlme_sleep2: Reaction ~ Days + (1 + Days | Subject)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_sleep1    4 1802.1 1814.8 -897.04   1794.1                         \nlme_sleep2    6 1763.9 1783.1 -875.97   1751.9 42.139  2  7.072e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nThis table gives us the chi-square statistic (i.e., the likelihood ratio) and an associated p-value.\n\nHere, we have a large chi-square statistic and a small p-value. This tells us that dropping the random slopes term from our model *does* have a significant effect - in other words, the random slopes term is meaningful and useful when predicting reaction times.\n\nWe can also test the significance of our fixed effect of `Days`. We keep the structure of random effects the same, and replace our fixed effect with `1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_random <- lmer(Reaction ~ 1 + (1 + Days|Subject), data = sleepstudy)\n\nanova(lme_sleep2, lme_random)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: sleepstudy\nModels:\nlme_random: Reaction ~ 1 + (1 + Days | Subject)\nlme_sleep2: Reaction ~ Days + (1 + Days | Subject)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_random    5 1785.5 1801.4 -887.74   1775.5                         \nlme_sleep2    6 1763.9 1783.1 -875.97   1751.9 23.537  1  1.226e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nOnce again, there is a significant difference between the two models, as seen by our small p-value. This tells us that the fixed effect of days is a significant predictor of reaction time.\n\n## Method 3: t-to-z approximations\n\nThe third and final method that we'll touch on in this course is another form of approximation - here, we make use of the Wald t-values, which are reported as standard in the `lme4` output. \n\nSpecifically, we can choose to treat these t-values as if they were z-scores instead, if our sample size is considered large enough. And, because z-scores are standardised, we don't need any degrees of freedom information to derive a p-value - we can just read them directly out of a table (or get R to do it for us).\n\n::: {.callout-note collapse=\"true\"}\n#### The logic of using z-scores instead\n\nA z-score is different from a statistic such as t or F. They're standardised, because they're measured in standard deviations - i.e., a z-score of 1.3 tells you that you are 1.3 standard deviations away from the mean. \n\nThis is helpful for deriving a p-value without degrees of freedom, but it raises the question: why is it okay to treat t-values as z-scores? \n\nThe logic here is that the t distribution actually begins to approximate (i.e., match up with) the z distribution as the sample size increases. Officially, when the sample size is infinite, the two distributions are identical. So, with a sufficiently large sample size, we can \"pretend\" or \"imagine\" that the Wald t-values are actually z-distributed, giving us p-values. \n:::\n\nUnfortunately, there are no formal guidelines to tell you whether your dataset is \"large enough\" to do this. It will depend on the number and type of predictors in your model. Plus, the t-to-z approximation is considered to be \"anti-conservative\" - in other words, there's a higher chance of false positives than with other methods.\n\nSome researchers adapt the t-to-z approximation approach a little to help with this; instead of explicitly calculating p-values, they instead use a rule of thumb that any Wald t-value greater than 2 is large enough to be considered significant. This is quite a strict threshold, so it can help to filter out some of the false positives or less convincing results.\n\n### Calculating p-values using z-scores\n\nLet's look again at the `summary` output for our model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lme_sleep2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Reaction ~ Days + (1 + Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  251.405      6.825  17.000  36.838  < 2e-16 ***\nDays          10.467      1.546  17.000   6.771 3.26e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n```\n:::\n:::\n\n:::\n\nCalculating the p-value for a z-score can be done quickly in R using the `pnorm` function. We include the z-score (or, here, the t-value that we are treating as a z-score) as the value for our argument `q`. To make this a two-tailed test, we have to set `lower.tail` to FALSE, and multiply the answer by 2.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n2*pnorm(q = 6.771, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.278953e-11\n```\n:::\n:::\n\n:::\n\nIf we input the t-value for our `Days` fixed effect, we can see that it gives us a very small p-value. This p-value of 1.28 x 10^-11^ is quite a bit smaller than the one that our Satterthwaite degrees of freedom approximation provided (3.26 x 10^-6^) - an example of how this t-to-z approximation is more generous. However, in this case it's very clear that the `Days` effect definitely is significant, whichever way we test it, so it's perhaps not a concern.\n\n#### And that, in fact, leads me to a final point I'd like to make: \n\nOne of the main reasons I've bothered to include all three of these methods here is because there's nothing stopping you using more than one approach when it comes to testing your own models. If you do use multiple methods, and they agree/lead you to the same conclusion, then that increases your overall confidence in that conclusion. If they disagree, then it's better that you know that your result might not be reliable, and you can do some further investigations to figure out why (e.g., perhaps it's a statistical power issue).\n\n## Exercises\n\n### Exercise 1 - Dragons revisited\n\n\n{{< level 2 >}}\n\n\n\nLet's return to the dataset from our previous example, our dragons dataset.\n\nPreviously, we fit a mixed model to this dataset that included response variable `intelligence`, fixed effects of `wingspan`, `scales` and `wingspan:colour`, and two random effects: random intercepts by `mountain`, and random slopes for `wingspan` by `mountain`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndragons <- read_csv(\"data/dragons.csv\")\n\nlme_dragons <- lmer(intelligence ~ wingspan*scales + (1 + wingspan|mountain), \n                    data=dragons)\n```\n:::\n\n:::\n\nNow, test the significance of this model and its parameters using the methods shown above. Think about:\n\n- whether any/all of our fixed effects are significant\n- whether either of our random effects are significant\n- whether the three methods lead you to the same conclusions, and if not, why this might be\n\n::: {.callout-note collapse=\"true\"}\n#### Worked answer\n\nLet's start by using an LRT to test the overall significance of our model.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# construct a null model\nlme_dragons_null <- lm(intelligence ~ 1, data = dragons)\n\n# compare the full and null models\nanova(lme_dragons, lme_dragons_null)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_null: intelligence ~ 1\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                 npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_dragons_null    2 1997.0 2003.6 -996.51   1993.0                         \nlme_dragons         8 1647.8 1674.2 -815.92   1631.8 361.18  6  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nIt's significant. Something in our model is doing something helpful. A really good start!\n\nNext, we can use LRTs to check the significance of our individual random effects. We'll build some new models - one with random intercepts only and one with random slopes only - and compare them to the full model. Remember that for these models to be comparable, we need to keep the fixed effects the same.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_dragons_dropslope <- lmer(intelligence ~ wingspan*scales + (1|mountain), \n                            data=dragons)\n\nlme_dragons_dropint <- lmer(intelligence ~ wingspan*scales + (0 + wingspan|mountain), \n                            data=dragons)\n```\n:::\n\n:::\n\nNow, we can compare both of these new models to our original model, using `anova`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropint)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_dropint: intelligence ~ wingspan * scales + (0 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                    npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropint    6 1643.9 1663.7 -815.95   1631.9                     \nlme_dragons            8 1647.8 1674.2 -815.92   1631.8 0.0691  2     0.9661\n```\n:::\n:::\n\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropslope)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_dropslope: intelligence ~ wingspan * scales + (1 | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropslope    6 1737.9 1757.7 -862.95   1725.9                     \nlme_dragons              8 1647.8 1674.2 -815.92   1631.8 94.057  2  < 2.2e-16\n                         \nlme_dragons_dropslope    \nlme_dragons           ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nThese results would seem to suggest that while the random slopes are significant, the random intercepts are not.\n\nThis opens up a bit of a debate about whether it's a good idea to drop a random effect from a model if it's not significant. You can, of course, use any model you like to describe your dataset. But if you're not pushed for statistical power, it can be worth leaving non-significant random effects in a model, so that it better reflects your experimental design. In this case, I would personally opt for leaving the random intercepts in my final model, because it's quite unusual to see random slopes on their own. But this is, of course, personal choice!\n\nLet's move on to thinking about our fixed effects. Following the same LRT procedure, we can construct some new models - one where we've dropped the `wingspan:scales` interaction, and two more where we exclude `wingspan` or `scales` entirely (plus the interaction in both cases). We'll keep the random effects the same across these models.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# testing the interaction term\nlme_dragons_dropx <- lmer(intelligence ~ wingspan + scales + (1 + wingspan|mountain), \n                          data=dragons)\n\nanova(lme_dragons, lme_dragons_dropx)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_dropx: intelligence ~ wingspan + scales + (1 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                  npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropx    7 1647.2 1670.3 -816.60   1633.2                     \nlme_dragons          8 1647.8 1674.2 -815.92   1631.8 1.3648  1     0.2427\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# testing the main effect of scales\nlme_dragons_dropwing <- lmer(intelligence ~ scales + (1 + wingspan|mountain), \n                            data=dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.003579 (tol = 0.002, component 1)\n```\n:::\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropwing)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_dropwing: intelligence ~ scales + (1 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                     npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropwing    6 1653.5 1673.2 -820.73   1641.5                     \nlme_dragons             8 1647.8 1674.2 -815.92   1631.8 9.6251  2   0.008127\n                       \nlme_dragons_dropwing   \nlme_dragons          **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# testing the main effect of wingspan\nlme_dragons_dropscale <- lmer(intelligence ~ wingspan + (1 + wingspan|mountain), \n                              data=dragons)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Model failed to converge with 1 negative eigenvalue: -2.6e+00\n```\n:::\n\n```{.r .cell-code}\nanova(lme_dragons, lme_dragons_dropscale)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrefitting model(s) with ML (instead of REML)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nData: dragons\nModels:\nlme_dragons_dropscale: intelligence ~ wingspan + (1 + wingspan | mountain)\nlme_dragons: intelligence ~ wingspan * scales + (1 + wingspan | mountain)\n                      npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\nlme_dragons_dropscale    6 1673.6 1693.3 -830.78   1661.6                     \nlme_dragons              8 1647.8 1674.2 -815.92   1631.8 29.724  2  3.512e-07\n                         \nlme_dragons_dropscale    \nlme_dragons           ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nTaken together, these LRTs suggest that although the interaction of `wingspan:scales` is not a particularly useful or significant predictor, the two main effects are.\n\nComfortingly, this aligns with what we see in an analysis of variance table using a Satterthwaite degrees of freedom approximation, which shows overall that there seem to be main effects though no significant interaction. The p-values are not the same - we wouldn't expect them to be, they're calculated very differently - but it's a relief that the overall effect is robust across methods:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lme_dragons)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nType III Analysis of Variance Table with Satterthwaite's method\n                 Sum Sq Mean Sq NumDF   DenDF F value  Pr(>F)   \nwingspan        3059.70 3059.70     1   3.992 16.8632 0.01484 * \nscales          1923.44 1923.44     1 188.765 10.6009 0.00134 **\nwingspan:scales  242.84  242.84     1 188.380  1.3384 0.24878   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n:::\n\nAnd, indeed, we would draw the same overall conclusion using t-to-z approximations as well (using the t-values, extracted from the output of the `summary` function). Excellent news.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# interaction term\n2*pnorm(q = -1.157, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.752728\n```\n:::\n\n```{.r .cell-code}\n# scales main effect\n2*pnorm(q = 3.256, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.001129938\n```\n:::\n\n```{.r .cell-code}\n# wingspan main effect\n2*pnorm(q = 4.244, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.195704e-05\n```\n:::\n:::\n\n:::\n\nOn the basis of these results, you could choose to refine your model slightly, eliminating the unhelpful `wingspan:scales` interaction. As discussed above, you could also choose to drop the random intercepts from the model, but that is a little more contentious. I'll keep it for now, making `lme_dragons_dropx` the working minimal model.\n\nWe can visualise that like so:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dragons, aes(x = wingspan, y = intelligence, colour = scales)) +\n  facet_wrap(vars(mountain)) +\n  geom_point() +\n  geom_line(data = augment(lme_dragons_dropx), aes(y = .fitted))\n```\n\n::: {.cell-output-display}\n![](significance-testing_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n:::\n\n:::\n\n### Exercise 2 - Irrigation revisited\n\n\n{{< level 2 >}}\n\n\n\nOnce again, we'll return to a dataset from the previous section of the course, and the model we fitted to it.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nirrigation <- read_csv(\"data/irrigation.csv\")\n\nlme_yield <- lmer(yield ~ irrigation*variety + (1|field), data = irrigation)\n```\n:::\n\n:::\n\nPerform model comparison to draw conclusions about which factors should be considered when maximising crop yield.\n\nThere's no worked answer for this exercise, but you can use the code from the `sleepstudy` and `dragons` examples to scaffold your work. Feel free to call over a trainer or chat to a neighbour if you want to compare results!\n\n## Summary\n\nThere are multiple ways to assess the significance of effects in a mixed effects model, which can aid you in determining which predictors are required in your analysis. This section introduces you to a few of them. \n\nThe most versatile of these methods - and one which is worth your time, even if the learning curve is slightly steeper - is likelihood ratio tests (LRTs). LRTs can be used for assessing the significance of random effects as well as fixed effects, and of the full model versus the null.\n\nIf you're interested in doing further reading on this topic, then [this article](https://link.springer.com/article/10.3758/s13428-016-0809-y) has a nice comparison of the methods discussed above, including how they perform in terms of type I (false positive) error rates.\n\n::: {.callout-tip}\n#### Key Points\n\n- Calculating p-values for mixed effects models is tricky, as there is no precise number of degrees of freedom\n- To calculate p-values for random and/or fixed effects, likelihood ratio tests can be used\n- Approximations of degrees of freedom (Satterthwaite and/or Kenward-Roger) or the t-as-z approximation can also be used to estimate p-values for fixed effects in mixed models\n:::\n\n",
    "supporting": [
      "significance-testing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
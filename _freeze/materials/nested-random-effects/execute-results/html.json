{
  "hash": "96935f2ede33fdb446e1de96b84d21a7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nested random effects\"\noutput: html_document\n---\n\n::: {.cell}\n\n:::\n\n\nMixed effects models are also sometimes referred to as \"hierarchical\" or \"multi-level\" models. So far in these materials, we've only fitted two-level models, containing a single clustering variable or effect. Sometimes, however, there are random effects nested *inside* others.\n\n## What is a nested random effect?\n\nOnce we are at the stage of having multiple variables in our sample that create clusters or groups, it becomes relevant to consider the relationship that those clustering variables have to one another, to ensure that we're fitting a model that properly represents our experimental design.\n\nWe describe factor B as being nested inside factor A, if each group/category of B only occurs within one group/category of factor A.\n\nFor instance, data on academic performance may be structured as children grouped within classrooms, with classrooms grouped within schools. A histology experiment might measure individual cells grouped within slices, with slices grouped within larger samples. Air pollution data might be measured at observation stations grouped within a particular city, with multiple cities per country.\n\n## Fitting a three-level model\n\nAnother classic example of nested random effects that would prompt a three-level model can be found in a clinical setting: within each hospital, there are multiple doctors, each of whom treats multiple patients. (Here, we will assume that each doctor only works at a single hospital, and that each patient is only treated by a single doctor.)\n\nHere's an image of how that experimental design looks. Level 1 is the patients, level 2 is the doctors, and level 3 is the hospitals. \n\nThis is, of course, a simplified version - we would hope that there are more than two hospitals, four doctors and eight patients in the full sample!\n\n![Experimental design](images_mixed-effects/nested-patients1.png){width=70%}\n\nWe have a single fixed predictor of interest `treatment` (for which there are two possible treatments, A or B), and some continuous response variable `outcome`.\n\nWhat model would we fit to these data? Well, it gets a touch more complex now that we have multiple levels in this dataset.\n\n### A three-level random intercepts model\n\nLet's put random slopes to one side, since they take a bit more thought, and think about how we would fit just some random intercepts for now.\n\nIt would be appropriate to fit two sets of random intercepts in this model, one for each set of clusters we have. In this case, that means a set of intercepts for the doctors, and a set of intercepts for the hospital.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth <- read_csv(\"data/health.csv\")\n```\n:::\n\n:::\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_health_intercepts <- lmer(outcome ~ treatment + (1|doctor) + (1|hospital),\n                   data = health)\n\nsummary(lme_health_intercepts)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ treatment + (1 | doctor) + (1 | hospital)\n   Data: health\n\nREML criterion at convergence: 1848.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.56989 -0.66086  0.06162  0.67602  2.84690 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n doctor   (Intercept)  2.1416  1.4634  \n hospital (Intercept)  0.1688  0.4108  \n Residual             26.3425  5.1325  \nNumber of obs: 300, groups:  doctor, 30; hospital, 5\n\nFixed effects:\n                 Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)       26.6155     0.5299   8.4431   50.23 9.34e-12 ***\ntreatmentsurgery   6.2396     0.5926 269.0000   10.53  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrtmntsrgry -0.559\n```\n\n\n:::\n:::\n\n:::\n\nThis produces a model with two random effects, namely, two sets of random intercepts.\n\n### Where to include random slopes?\n\nDeciding what level(s) we want to fit random slopes at, requires us to think about what level of our hierarchy we've applied our `Treatment` variable at. We'll get to that in a moment.\n\n#### Predictor varies at level 1\n\nLet's start by imagining the following scenario: the `Treatment` variable is varying at our lowest level. Each patient receives only one type of treatment (A or B), but both treatment types are represented \"within\" each doctor and within each hospital:\n\n![Scenario 1: predictor varies at level 1 (between patients, within doctors)](images_mixed-effects/nested-patients2.png){width=70%}\n\nAs a result, it would be inappropriate to ask `lme4` to fit random slopes for the `treatment` variable at the patient level. Instead, the \"full\" model (i.e., a model containing all of the possible fixed and random effects) would be the following:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_health_slopes <- lmer(outcome ~ treatment + (1 + treatment|doctor) + \n                      (1 + treatment|hospital), data = health)\n\nsummary(lme_health_slopes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ treatment + (1 + treatment | doctor) + (1 + treatment |  \n    hospital)\n   Data: health\n\nREML criterion at convergence: 1834.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.38341 -0.60702 -0.02763  0.71956  2.88951 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n doctor   (Intercept)       7.8846  2.8080        \n          treatmentsurgery  7.3157  2.7047   -0.96\n hospital (Intercept)       0.5085  0.7131        \n          treatmentsurgery  3.5595  1.8867   -0.91\n Residual                  23.5775  4.8557        \nNumber of obs: 300, groups:  doctor, 30; hospital, 5\n\nFixed effects:\n                 Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)       26.6155     0.7223  4.0083  36.849 3.17e-06 ***\ntreatmentsurgery   6.2396     1.1270  3.9992   5.537  0.00521 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrtmntsrgry -0.794\n```\n\n\n:::\n:::\n\n:::\n\nThis produces a model with four sets of random effects: two sets of random intercepts, and two sets of random slopes.\n\n#### Predictor varies at level 2\n\nLet's now imagine a (perhaps more realistic) scenario. Each doctor is in fact a specialist in a certain type of treatment, but cannot deliver both. For this, we will need to read in the second version of our dataset.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth2 <- read_csv(\"data/health2.csv\")\n```\n:::\n\n:::\n\nIf you look closely at the dataset, you can see that `treatment` does not vary within `doctor`; instead, it only varies within `hospital`. \n\n![Scenario 2: predictor varies at level 2 (between doctors, within hospitals)](images_mixed-effects/nested-patients3.png){width=70%}\n\nThis means we cannot fit random slopes for treatment at the second level any more. We have to drop our random slopes for `treatment` by `doctor`, like this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_health_slopes2 <- lmer(outcome ~ treatment + (1|doctor) + \n                      (1 + treatment|hospital), data = health2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nboundary (singular) fit: see help('isSingular')\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(lme_health_slopes2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ treatment + (1 | doctor) + (1 + treatment | hospital)\n   Data: health2\n\nREML criterion at convergence: 1845.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.46342 -0.69900 -0.05043  0.69559  3.09601 \n\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n doctor   (Intercept)       6.318   2.514         \n hospital (Intercept)       1.372   1.171         \n          treatmentsurgery  3.635   1.907    -1.00\n Residual                  24.417   4.941         \nNumber of obs: 300, groups:  doctor, 30; hospital, 5\n\nFixed effects:\n                 Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)       25.7019     0.9265  4.3745  27.740 4.38e-06 ***\ntreatmentsurgery   4.5061     1.3766  4.0456   3.273   0.0302 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\ntrtmntsrgry -0.808\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n```\n\n\n:::\n:::\n\n:::\n\nThis gives us three sets of random effects, as opposed to four.\n\n#### Predictor varies at level 3\n\nFinally, let's imagine a scenario where each hospital is only equipped to offer one type of treatment (hopefully, not a realistic scenario!). Here, all doctors and patients within each hospital use exactly the same method. \n\n![Scenario 3: predictor varies at level 3 (between hospitals)](images_mixed-effects/nested-patients4.png){width=70%}\n\nAt this stage, we can no longer include random slopes for the treatment predictor anywhere in our model. Each `hospital`, `doctor` and `patient` only experiences one of the two treatments, not both, so we have no variation between the treatments to estimate at any of these levels.\n\nSo, here, we would go back to our random intercepts only model.\n\n## Implicit vs explicit nesting\n\nThe different `health` datasets that have been explored above all have an important thing in common: the variables have been **implicitly nested**. Each new hospital, doctor and patient is given a unique identifier, to make it clear that doctors do not reoccur between hospitals, and patients do not reoccur between doctors. \n\nIn other words, all the information about the nesting is captured implicitly in the way that the data are coded.\n\nHowever, you might sometimes be working with a dataset that has not been coded this way. So how do you deal with those situations? You have a few options:\n\n- Recode your dataset so it is implicitly nested\n- Use explicit nesting in your `lme4` model formula\n- Use the `A/B` syntax in your `lme4` model formula\n\n### The Pastes dataset\n\nWe'll use another internal `lme4` dataset, the `Pastes` dataset, to show you what these three options look like in practice.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Pastes\")\n\nhead(Pastes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  strength batch cask sample\n1     62.8     A    a    A:a\n2     62.6     A    a    A:a\n3     60.1     A    b    A:b\n4     62.3     A    b    A:b\n5     62.7     A    c    A:c\n6     63.1     A    c    A:c\n```\n\n\n:::\n:::\n\n:::\n\nThis dataset is about measuring the strength of a chemical paste product, which is delivered in batches, each batch consisting of several casks. From ten random deliveries of the product, three casks were chosen at random (for a total of 30 casks). A sample was taken from each cask; from each sample, there were two assays, for a total of 60 assays.\n\nThere are four variables: \n\n- `strength`, paste strength, a continuous response variable; measured for each assay\n- `batch`, delivery batch from which the sample was chosen (10 groups, A to J)\n- `cask`, cask within the deliver batch from which the sample was chosen (3 groups, a to c)\n- `sample`, batch & cask combination (30 groups, A:a to J:c)\n\nThe experimental design, when drawn out, looks like this:\n\n![Pastes dataset design](images_mixed-effects/pastes_design.png){width=70%}\n\nAt first glance, this might look as if it's a four-level model: assays within samples within casks within deliveries. However, that's a bit of a overcomplication. There is only one `sample` collected per `cask`, meaning that we can really just think about assays being nested within casks directly.\n\nThe aim of this experiment was simply to assess the average strength of the chemical paste, adjusting for variation across batches and casks. Therefore, there are no fixed effects in the model - instead, we simply write `1` for the fixed portion of our model. We also won't have any random slopes, only random intercepts. \n\nIf we follow the same procedure we did above for the `health` example, we might try something like this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_paste <- lmer(strength ~ 1 + (1|batch) + (1|cask), data = Pastes)\n\nsummary(lme_paste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: strength ~ 1 + (1 | batch) + (1 | cask)\n   Data: Pastes\n\nREML criterion at convergence: 301.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.49025 -0.90096 -0.01247  0.62911  1.82246 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n batch    (Intercept) 3.3639   1.8341  \n cask     (Intercept) 0.1487   0.3856  \n Residual             7.3060   2.7030  \nNumber of obs: 60, groups:  batch, 10; cask, 3\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  60.0533     0.7125  6.7290   84.28 1.99e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nSomething is wrong with this model. To spot it, we have to look carefully at the bottom of the random effects section, where it says `Number of obs` (short for observations). \n\n`lme4` has correctly identified that there are 10 delivery batches, and has fitted a set of 10 random intercepts for those batches - all good so far. However, R believes that we only have 3 casks, because the `cask` variable is implicitly nested, and so has only fitted a set of 3 random intercepts for that variable. \n\nBut this isn't what we want. There is no link between cask A in batch A, and cask A in batch D - they have no reason to be more similar to each other than they are to other casks. We actually have 30 unique casks, and would like for each of them to have its own random intercept.\n\n### Recoding for implicit nesting\n\nAs shown above, the formula `strength ~ 1 + (1|batch) + (1|cask)` does not produce the model we want, because we don't have implicit coding in the `cask` variable.\n\nSo, let's create a new variable that gives unique values to each of the casks in our dataset. We'll do this using the `mutate` function and the `paste()` function to create a unique ID for each cask.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nPastes <- Pastes %>% mutate(unique_cask = paste(batch, cask, sep = \"_\"))\n\nhead(Pastes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  strength batch cask sample unique_cask\n1     62.8     A    a    A:a         A_a\n2     62.6     A    a    A:a         A_a\n3     60.1     A    b    A:b         A_b\n4     62.3     A    b    A:b         A_b\n5     62.7     A    c    A:c         A_c\n6     63.1     A    c    A:c         A_c\n```\n\n\n:::\n:::\n\n:::\n\nThis generates 30 unique IDs, one for each of our unique casks. (We then have two observations of `strength` for each `unique_cask`.)\n\nNow, we can go ahead and fit our desired model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_paste_implicit <- lmer(strength ~ 1 + (1|batch) + (1|unique_cask),\n                  data = Pastes)\n\nsummary(lme_paste_implicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: strength ~ 1 + (1 | batch) + (1 | unique_cask)\n   Data: Pastes\n\nREML criterion at convergence: 247\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4798 -0.5156  0.0095  0.4720  1.3897 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n unique_cask (Intercept) 8.434    2.9041  \n batch       (Intercept) 1.657    1.2874  \n Residual                0.678    0.8234  \nNumber of obs: 60, groups:  unique_cask, 30; batch, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  60.0533     0.6769  9.0000   88.72 1.49e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nNo error message this time, and it has correctly identified that there are 30 unique casks, from 10 different batches. We've solved the problem!\n\nIncidentally, and which you may have already noticed, the recoding that we did above also perfectly replicates the existing `sample` variable. This means we would get an identical result if we fitted the model `strength ~ 1 + (1|batch) + (1|sample)` instead. \n\n### Fitting a model with explicit nesting\n\nIf we're in a situation like the above where we don't have nice, neat implicitly coded variables, but we don't really want to spend loads of time recoding a bunch of variables, we can instead fit our model using explicit nesting in `lme4`.\n\nThat essentially means combining the recoding and model fitting steps, so that you don't have to save a new variable.\n\nFor the `Pastes` dataset, it would look like this:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_paste_explicit <- lmer(strength ~ 1 + (1|batch) + (1|batch:cask), data = Pastes)\n\nsummary(lme_paste_explicit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: strength ~ 1 + (1 | batch) + (1 | batch:cask)\n   Data: Pastes\n\nREML criterion at convergence: 247\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4798 -0.5156  0.0095  0.4720  1.3897 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n batch:cask (Intercept) 8.434    2.9041  \n batch      (Intercept) 1.657    1.2874  \n Residual               0.678    0.8234  \nNumber of obs: 60, groups:  batch:cask, 30; batch, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  60.0533     0.6769  9.0000   88.72 1.49e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nNotice that the output for this model and for the one right above are the same - it's because they are exactly equivalent to each other! We used `batch:cask` to create `unique_cask` earlier, so we've just directly inserted that code into our formula.\n\n### An alternative approach: the A/B syntax\n\nThere is another way to deal with nested random effects that haven't been implicitly coded into the dataset. It's not necessarily the way that we would recommend - recoding your variables and/or using explicit nesting has far less potential to trip you up and go wrong - but we'll introduce it briefly here, since it's something you're likely to see if you start working with mixed models a lot.\n\nWe could fit the same model to the `Pastes` dataset, and achieve a set of 30 intercepts for `cask` and 10 for `batch`, without making use of the `sample` variable or using the `A:B` notation.\n\nIt works like this: when random effect B is nested inside random effect A, you can simply write `A/B` on the right hand side of the `|`.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_paste_shorthand <- lmer(strength ~ 1 + (1|batch/cask), data = Pastes)\n\nsummary(lme_paste_shorthand)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: strength ~ 1 + (1 | batch/cask)\n   Data: Pastes\n\nREML criterion at convergence: 247\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.4798 -0.5156  0.0095  0.4720  1.3897 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n cask:batch (Intercept) 8.434    2.9041  \n batch      (Intercept) 1.657    1.2874  \n Residual               0.678    0.8234  \nNumber of obs: 60, groups:  cask:batch, 30; batch, 10\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  60.0533     0.6769  9.0000   88.72 1.49e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n:::\n\nThis gives you an identical model output to both of the other two models we've tried. That's because `(1|batch/cask)` is actually shorthand for `(1|batch) + (1|batch:cask)`, and in this dataset as we've seen above, `batch:cask` is the same as `sample` and `unique_cask`. In other words, we've fitted exactly the same model each time.\n\n### Which option is best?\n\n::: {.callout-note appearance=\"minimal\"}\nThe `A/B` syntax is popular, and you will see it used, because it's quick to write. But there are a couple of reasons that we would recommend you steer away from it:\n\n- **It has more potential to go wrong.** `A/B` is order-dependent, meaning that `A/B` $\\neq$ `B/A`, and if you get that wrong, your model won't work as intended. In contrast, explicit coding is order-invariant (`A:B + A` = `A + B:A`). Likewise, if you've implicitly coded your dataset, you can write your random effects in any order.\n\n- **It's harder to interpret.** Using implicit or explicit nesting gives you a separate `(1 + x|y)` structure in the formula for each clustering variable, which is more transparent when figuring out how many random effects you've fitted. Separate structures can also be more flexible, as you'll see in at least one example later in the course.\n:::\n\nOverall: implicit coding of your dataset is best, and it's best to do this implicit coding during data collection itself. It prevents mistakes, because the experimental design is clear from the data structure. It's also the easiest and most flexible in terms of coding in `lme4`.\n\nIn the absence of an implicitly coded dataset, we strongly recommend sticking with explicit coding rather than the shorthand syntax - yes, it requires more typing, but it's somewhat safer.\n\nAnd, no matter which method you choose, always check the model output to see that the number of groups per clustering variables matches what you expect to see.\n\n## Exercises\n\n### Cake {#sec-exr_cake}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise, we'll use the most delicious of the internal `lme4` datasets: `cake`.\n\nThis is a real dataset, taken from a thesis by Cook ([1983](https://www.sumsar.net/blog/source-of-the-cake-dataset/cook_1938_chocolate_cake.pdf)), and is all about measuring cake quality of chocolate cakes baked using different recipes at particular temperatures.\n\nCook's experiment worked as follows: for each recipe, she prepared 15 batches of cake mixture. Each batch was divided into 6 cakes, and each of those cakes were baked at one of the 6 temperatures.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"cake\")\n```\n:::\n\n:::\n\nThere are five variables in this dataset:\n\n- `recipe`, the exact recipe used for the cake (3 categories)\n- `replicate`, the batter batch number for each recipe (15 replicates per recipe)\n- `temperature`, a factor indicating which of 6 temperatures the cake was baked at\n- `temp`, the numeric value of the baking temperature\n- `angle`, the angle at which the cake broke (used as a measure of cake \"tenderness\")\n\nFor this exercise:\n\n1. Sketch a graphic/diagram that captures the experimental design\n2. Figure out what level of the dataset your variables of interest are varying at\n3. Consider how you might recode the dataset to reflect implicit nesting\n4. Fit and test at least one appropriate model\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\n#### Consider the experimental design\n\nThe first thing to do is to draw out a diagram that captures the experimental design. That might look something like this:\n\n![Experimental design for Cook's cakes](images_mixed-effects/cake_design.png){width=70%}\n\nWe've got a 3-level dataset here: individual cakes within batches (replicates) within recipes. There are two fixed effects of interest - `recipe` and `temperature`, both categorical.\n\nThe next thing to think about is whether the coding within the dataset accurately reflects what's going on.\n\nThere are 3 recipes, and 15 replicates of each recipe are mixed, for a total of 45 unique batches or mixtures. In the dataset as we've got it, the numbering has been repeated, so the nesting is not implicitly coded; but of course, replicate 1 for recipe A doesn't have anything more in common with replicate 1 for recipe C.\n\n#### Recode the dataset\n\nSo, let's code up a new variable that captures unique replicates, and we'll call it `batch`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncake <- cake %>%\n  mutate(batch = recipe:replicate)\n```\n:::\n\n:::\n\nAs you should be able to see in your global environment, the new `batch` variable is a factor with 45 levels. \n\nEach batch is then split into individual cakes, which undergo one of the 6 `temperature` treatments, for a total of 270 measurements.\n\n#### Fit a model\n\nNow, we can try fitting a model. We know that we want `recipe` and `temperature` as fixed effects, and probably also their interaction, at least to start with. We know we want to treat `batch` as a random effect (`replicate` nested within `recipe`), so we'll include random intercepts.\n\nWe don't, however, want to treat `recipe` as a random effect itself. It only has three levels, so it wouldn't work well even if we wanted to. Plus, we're specifically interested in those three levels and the differences between them.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake <- lmer(angle ~ recipe*temperature + (1|batch), data = cake)\n\nsummary(lme_cake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: angle ~ recipe * temperature + (1 | batch)\n   Data: cake\n\nREML criterion at convergence: 1638.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.64661 -0.61082 -0.05207  0.56985  2.75374 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n batch    (Intercept) 41.84    6.468   \n Residual             20.47    4.524   \nNumber of obs: 270, groups:  batch, 45\n\nFixed effects:\n                       Estimate Std. Error        df t value Pr(>|t|)    \n(Intercept)            33.12222    1.73683  42.00000  19.070  < 2e-16 ***\nrecipeB                -1.47778    2.45625  42.00000  -0.602  0.55065    \nrecipeC                -1.52222    2.45625  42.00000  -0.620  0.53878    \ntemperature.L           6.43033    1.16822 210.00000   5.504 1.07e-07 ***\ntemperature.Q          -0.71285    1.16822 210.00000  -0.610  0.54239    \ntemperature.C          -2.32551    1.16822 210.00000  -1.991  0.04782 *  \ntemperature^4          -3.35128    1.16822 210.00000  -2.869  0.00454 ** \ntemperature^5          -0.15119    1.16822 210.00000  -0.129  0.89715    \nrecipeB:temperature.L   0.45419    1.65211 210.00000   0.275  0.78365    \nrecipeC:temperature.L   0.08765    1.65211 210.00000   0.053  0.95774    \nrecipeB:temperature.Q  -0.23277    1.65211 210.00000  -0.141  0.88809    \nrecipeC:temperature.Q   1.21475    1.65211 210.00000   0.735  0.46299    \nrecipeB:temperature.C   2.69322    1.65211 210.00000   1.630  0.10456    \nrecipeC:temperature.C   2.63856    1.65211 210.00000   1.597  0.11175    \nrecipeB:temperature^4   3.02372    1.65211 210.00000   1.830  0.06863 .  \nrecipeC:temperature^4   3.13711    1.65211 210.00000   1.899  0.05895 .  \nrecipeB:temperature^5  -0.66354    1.65211 210.00000  -0.402  0.68836    \nrecipeC:temperature^5  -1.62525    1.65211 210.00000  -0.984  0.32637    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nCorrelation matrix not shown by default, as p = 18 > 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n```\n\n\n:::\n:::\n\n:::\n\n#### Alternative models\n\nIf you want to do a bit of significance testing, you can try a few other versions of the model with different structures:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_cake2 <- lmer(angle ~ recipe + temperature + (1|batch), data = cake)\nlme_cake3 <- lmer(angle ~ recipe + (1|batch), data = cake)\nlme_cake4 <- lmer(angle ~ temperature + (1|batch), data = cake)\nlm_cake <- lm(angle ~ recipe*temperature, data = cake)\n\nanova(lme_cake, lm_cake) # random effects dropped\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: cake\nModels:\nlm_cake: angle ~ recipe * temperature\nlme_cake: angle ~ recipe * temperature + (1 | batch)\n         npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlm_cake    19 1901.3 1969.6 -931.63   1863.3                         \nlme_cake   20 1719.0 1791.0 -839.53   1679.0 184.21  1  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_cake, lme_cake2) # recipe:temperature dropped\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: cake\nModels:\nlme_cake2: angle ~ recipe + temperature + (1 | batch)\nlme_cake: angle ~ recipe * temperature + (1 | batch)\n          npar    AIC    BIC  logLik deviance Chisq Df Pr(>Chisq)\nlme_cake2   10 1709.6 1745.6 -844.79   1689.6                    \nlme_cake    20 1719.0 1791.0 -839.53   1679.0 10.53 10     0.3953\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_cake, lme_cake3) # temperature & recipe:temperature dropped\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: cake\nModels:\nlme_cake3: angle ~ recipe + (1 | batch)\nlme_cake: angle ~ recipe * temperature + (1 | batch)\n          npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nlme_cake3    5 1785.7 1803.7 -887.84   1775.7                         \nlme_cake    20 1719.0 1791.0 -839.53   1679.0 96.636 15  5.642e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(lme_cake, lme_cake4) # recipe & recipe:temperature dropped\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrefitting model(s) with ML (instead of REML)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nData: cake\nModels:\nlme_cake4: angle ~ temperature + (1 | batch)\nlme_cake: angle ~ recipe * temperature + (1 | batch)\n          npar    AIC    BIC  logLik deviance Chisq Df Pr(>Chisq)\nlme_cake4    8 1706.1 1734.9 -845.06   1690.1                    \nlme_cake    20 1719.0 1791.0 -839.53   1679.0 11.06 12     0.5238\n```\n\n\n:::\n:::\n\n:::\n\nWe see that when we drop the random intercepts for `batch`, and when we drop the `temperature` predictor, our chi-square values are significant. This indicates that these predictors are important. But we can drop `recipe` and `recipe:temperature` without a particularly big change in deviance.\n\n(This is borne out somewhat if you've used the `lmerTest` package to perform degrees of freedom approximation and extract p-values as part of the `lme_cake` model summary.)\n\nSo, our final model is probably `lme_cake4 = angle ~ temperature + (1|batch)`. \n\n#### Check assumptions\n\nFor completeness, we'll check the assumptions of `lme_cake4` and visualise it for the sake of aiding our interpretation.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_cake4, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_cake4, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n:::\n\n:::\n\nMost of the assumptions look okay, with the exception of the normal Q-Q plot for the random intercepts. The set of intercepts doesn't really look like it's nicely normally distributed here. Maybe a more complicated mixed effects model (something beyond the linear type we're going with here) would help. Or, maybe this just means we should be a little less decisive in our overall conclusions.\n\n#### Visualise the model\n\nLast but not least, we can visualise the model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_cake4), aes(x = temperature, y = angle)) +\n  geom_point(alpha = 0.7) +\n  geom_line(aes(y = .fitted, group = batch), alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n:::\n\nOverall, `angle` increases with `temperature`. (From what I understand of reading the thesis, this is a good thing from the perspective of the cake quality, as it suggests the cake is more tender. Scientific *and* delicious.)\n\nWe can see visually that the `recipe` and `recipe:temperature` terms don't have much explanatory power by visualising the full model (commenting out the `facet_wrap` may also help you to see this):\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_cake), aes(x = temperature, y = angle, colour = recipe)) +\n  facet_wrap(~ recipe) +\n  geom_point(alpha = 0.7) +\n  geom_line(aes(y = .fitted, group = batch))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n:::\n\n:::\n\n:::\n\n::: {.callout-exercise}\n#### Bonus questions\n\n\n{{< level 2 >}}\n\n\n\nIf you want to think a bit harder about this dataset, consider these additional questions. Feel free to chat about them with a neighbour or with a trainer.\n\n- Why doesn't it work if you try to fit random slopes for `temperature` on `batch`? Have a look at the warning message that R gives you in this situation.\n- What happens if you use the numerical `temp` variable instead of the categorical `temperature`? Does it change your conclusions? Why might you prefer to use the numerical/continuous version?\n- Could `temperature` be treated as a random effect, under certain interpretations of the original research question? Is it possible or sensible to do that with the current dataset?\n:::\n\nFor more information on the very best way to bake a chocolate cake (and a lovely demonstration at the end about the dangers of extrapolating from a linear model), [this blog post](https://www.sumsar.net/blog/source-of-the-cake-dataset/) is a nice source. It's written by a data scientist who was so curious about the quirky `cake` dataset that he contacted Iowa State University, who helped him unearth Cook's original thesis.\n\n### Parallel fibres {#sec-exr_parallel}\n\n::: {.callout-exercise}\n\n\n{{< level 2 >}}\n\n\n\nFor this exercise, we'll be using a neurohistology dataset that focuses on a particular type of neuron found in the cerebellum, known as a parallel fibre. Parallel fibres are found in the uppermost layer of cerebellar cortex, and are known for being long; this experiment was designed to test whether the depth at which the fibre was found, had a bearing on its length.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel <- read_csv(\"data/parallel.csv\")\n```\n:::\n\n:::\n\nTo measure the length of the fibres (which are <10mm long - big for a neuron!), slices were taken from the cerebella of six cats, at different depths. The depth of each slice was recorded. These slices were then stained, so that individual parallel fibres could be identified and measured.\n\n![An example of a stained slice from cerebellar cortex](images_mixed-effects/cerebellum_histology.jpg){width=40%}\n\nThe dataset contains five variables:\n\n- `length` of the fibre (in micrometres)\n- `depth` of the slice (in micrometres)\n- `fibre`, individual IDs for all of the fibres\n- `slice` ID number (maximum 10 slices per cat)\n- `cat` ID number (1 through 6)\n\nFor this exercise:\n\n1. Sketch a graphic/diagram that captures the experimental design\n2. Determine whether the dataset requires recoding or explicit nesting\n3. Fit and test at least one appropriate model\n\n::: {.callout-tip collapse=\"true\"}\n#### Worked answer\n\n#### Visualise the design\n\nThis is a nested design with three levels: `fibre` within `slice` within `cat`. But the fixed predictor `depth` varies at level 2, between slices (not between fibres).\n\n![Experimental design](images_mixed-effects/neurohist_design.png){width=70%}\n\n#### Recoding\n\nIf we look at the structure of the dataset, we can see that the numbering for the `slice` variable starts again for each `cat` at 1, which is not what we want.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel %>% slice(1:8, 46)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  fibre length depth slice   cat\n  <dbl>  <dbl> <dbl> <dbl> <dbl>\n1     1   5780   260     1     1\n2     2   5730   260     1     1\n3     3   5790   260     1     1\n4     4   5860   260     1     1\n5     5   5690   260     1     1\n6     6   5940   260     1     1\n7     7   5950   260     1     1\n8     8   5940   290     2     1\n9    46   4670   300     1     2\n```\n\n\n:::\n:::\n\n:::\n\nSo, we need to recode a new variable, or be prepared to use explicit nesting in our model formula. Note that for this recoding to work, we also need to ask R to treat `slice` and `cat` as factors rather than numeric variables.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nparallel <- parallel %>%\n  mutate(cat = as.factor(cat)) %>%\n  mutate(slice = as.factor(slice)) %>%\n  mutate(unique_slice = slice:cat)\n```\n:::\n\n:::\n\n#### Fit a model\n\nThe full model that we could fit to these data contains three random effects: random intercepts for `unique_slice` (or `slice:cat` if you're explicitly coding), random intercepts for `cat`, and random slopes for `depth` on `cat`.\n\n(Since `depth` doesn't vary within `slice`, i.e., each `slice` has only one `depth`, we can't fit random slopes at level 2.)\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_parallel <- lmer(length ~ depth + (1|slice:cat) + (1 + depth|cat), \n                     data = parallel)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 1.2981 (tol = 0.002, component 1)\n```\n\n\n:::\n:::\n\n:::\n\nYou may notice that you get an error - the model fails to converge. There are a couple of fixes we could try that involve tweaking the settings in the estimation procedure (e.g., increasing the maximum number of iterations allowed).\n\nHowever, most errors like this just mean that we're being too ambitious. So, the approach we'll take here is to make the model simpler. \n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nlme_parallel_int <- lmer(length ~ depth + (1|slice:cat) + (1|cat), \n                         data = parallel)\n```\n:::\n\n:::\n\n#### Check the assumptions\n\nNext, we check the assumptions of our intercepts-only nested model:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(lme_parallel_int, \n            check = c(\"linearity\", \"homogeneity\", \"qq\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncheck_model(lme_parallel_int, \n            check = c(\"reqq\", \"pp_check\"))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-25-2.png){width=672}\n:::\n:::\n\n:::\n\nNot bad at all. There are no obvious errors cropping up in these plots.\n\n#### Visualise the model\n\nLast but not least, we should have a look at our model predictions visually.\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(augment(lme_parallel_int), aes(x = depth, y = length, colour = cat)) +\n  geom_point(alpha = 0.6) +\n  geom_line(aes(y = .fitted, group = cat))\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n:::\n\nDespite the fact that `depth` is a continuous variable, this plot still has jagged, rather than straight, lines of best fit. This is because the plot is also taking into account the multiple sets of random intercepts for `slice` that are contained within each `cat` cluster.\n\nWe can, however, extract just the set of intercepts by `cat`, and with a bit more fuss, use this to add lines to the plot with `geom_abline`:\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# use the coef function to extract the coefficients\ncat_coefs <- coef(lme_parallel_int)$cat\n\n# use geom_abline to add individual lines for each cat\nggplot(augment(lme_parallel_int), aes(x = depth, y = length, colour = cat)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(intercept = cat_coefs[1,1], slope = cat_coefs[1,2]) +\n  geom_abline(intercept = cat_coefs[2,1], slope = cat_coefs[2,2]) +\n  geom_abline(intercept = cat_coefs[3,1], slope = cat_coefs[3,2]) +\n  geom_abline(intercept = cat_coefs[4,1], slope = cat_coefs[4,2]) +\n  geom_abline(intercept = cat_coefs[5,1], slope = cat_coefs[5,2]) +\n  geom_abline(intercept = cat_coefs[6,1], slope = cat_coefs[6,2])\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n:::\n\n#### Is this a good model?\n\n::: {.panel-tabset group=\"language\"}\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(parallel, aes(x = depth, y = length, colour = slice)) +\n  facet_wrap(~cat) +\n  geom_point(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](nested-random-effects_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n:::\n\nIf we plot the data faceted by `cat`, it suggests that the relationship between `depth` and `length` varies between `cat`. But we were forced to drop the random slopes for `depth|cat` due to lack of model convergence.\n\nOur diagnostic plots look pretty good for our simpler, intercepts-only model, but the raw data indicate we might be missing something. Do you still trust the model? If no, what might a researcher do to improve this analysis?\n\n:::\n\n:::\n\n::: {.callout-exercise}\n#### Bonus question: notation\n\n\n{{< level 3 >}}\n\n\n\nThink back to the brief introduction to linear mixed effects models notation given in section 5.5.1 of the course materials.\n\nWhat would the equation of a three level model fitted to the `parallel` dataset look like?\n\nHint: you'll need more subscript letters than you did for a two-level model!\n\n::: {.callout-tip collapse=\"true\"}\n#### Answer: three-level intercepts-only\n\nE.g., `length ~ depth + (1|slice:cat) + (1|cat)`\n\nLevel 1:\n\n$$\ny_{ijk} = \\beta_{0jk} + \\beta_{1}x_{1ijk} + \\epsilon_{ijk}\n$$\n\nLevel 2: \n\n$$\n\\beta_{0jk} = \\delta_{00k} + U_{0jk}\n$$\n\nLevel 3: \n\n$$\n\\delta_{00k} = \\gamma_{000} + V_{00k}\n$$\n\nwhere,\n\n$$\n\\left( \\begin{array}{c} U_{0jk} \\end{array} \\right) ∼ N \\left( \\begin{array}{c} 0 \\end{array}   , \\begin{array}{cc} \\tau^2_{00} \\end{array} \\right)\n$$\n\nand,\n\n$$\n\\left( \\begin{array}{c} V_{00k} \\end{array} \\right) ∼ N \\left( \\begin{array}{c} 0 \\end{array}   , \\begin{array}{cc} \\tau^2_{00} \\end{array} \\right)\n$$\n\n:::\n\n::: {.callout-tip collapse=\"true\"}\n#### Answer: three-level intercepts & slopes\n\nE.g., `length ~ depth + (1|slice:cat) + (1 + depth|cat)`\n\nLevel 1:\n\n$$\ny_{ijk} = \\beta_{0jk} + \\beta_{1k}x_{1ijk} + \\epsilon_{ijk}\n$$\n\nLevel 2: \n\n$$\n\\beta_{0jk} = \\beta_{00k} + U_{0jk}\n$$\n\nLevel 3: \n\n$$\n\\beta_{00k} = \\gamma_{000} + V_{00k}\n$$\n$$\n\\beta_{1k} = \\gamma_{100} + V_{10k}\n$$\n\nWhere,\n\n$$\n\\left( \\begin{array}{c} U_{0jk} \\end{array} \\right) ∼ N \\left( \\begin{array}{c} 0 \\end{array}   , \\begin{array}{cc} \\tau^2_{00} \\end{array} \\right)\n$$\n\nand,\n\n$$\n\\left( \\begin{array}{c} V_{00k} \\\\ V_{10k} \\end{array} \\right) ∼ N \\left( \\begin{array}{c} 0 \\\\ 0 \\end{array}   , \\begin{array}{cc} \\tau^2_{00} & \\rho_{01} \\\\ \\rho_{01} &  \\tau^2_{10} \\end{array} \\right)\n$$\n\n:::\n\n:::\n\n\n## Summary\n\nSometimes, a dataset contains multiple clustering variables. When one of those clustering variables is nested inside the other, we can model this effectively by estimating random effects at multiple levels. \n\nAdding additional levels can create some complications, e.g., determining which level of the dataset your predictor variables are varying at. But it can also allow us to deal with real-life hierarchical data structures, which are common in research.\n\n::: {.callout-tip}\n#### Key points\n- Random effect B is nested inside random effect A, if each category of B occurs uniquely within only one category of A\n- It's important to figure out what level of the hierarchy or model a predictor variable is varying at, to determine where random slopes are appropriate\n- Nested random effects can be implicitly or explicitly coded in a dataframe, which determines how the model should be specified in `lme4`\n:::\n\n",
    "supporting": [
      "nested-random-effects_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}